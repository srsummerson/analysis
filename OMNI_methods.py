import numpy as np
import scipy as sp
from scipy import stats
from scipy import io
from scipy import signal
import re
#from neo import io
import pandas as pd
from matplotlib import pyplot as plt
import matplotlib as mpl
from matplotlib import mlab
import tables
from pylab import specgram
import time
from rt_calc import compute_rt_per_trial_CenterOut




def plotRawLFPTraces(data, **kwargs):
	'''
	This method plots the raw LFP data for all channels or a subset of channels in a single plot 
	for easy viewing. Data is normalized to have zero DC component. Spacing between traces is determined by 
	the max standard deviation across all channels.

	Input:
		- data: numpy array, as generated by the convert_OMNI method
	Optional input:
		- channs: list of channels to be plotted
		- filter_data: True/False if low-pass filter is applied to data, default is False
	'''
	num_channs = data.shape[1]
	 		
	if kwargs:
		channs = kwargs['channs']
	else:
		channs = range(1,num_channs+1,1) 	# recall one column is for timestamps, not channel data

	channs = [(chann - 1) for chann in channs]  # channel numbers are offset by 1 from index values

	'''
	if filter_data:
		cutoff_f = 100
		cutoff_f = cutoff_f/(1000./2)  # sampling rate is 1000 Hz
		num_taps = 100
		lpf = signal.firwin(num_taps,cutoff_f,window='hamming')
		for chann in channs:
			data[:][chann] = signal.lfilter(lpf,1,data[:][chann])
	'''

	mean_vec = np.mean(data[:,channs], axis = 0)
	std_vec = np.std(data[:,channs], axis = 0)

	for i, chann in enumerate(channs):
		test_above_thres = np.ravel(np.nonzero(np.greater(data[:,chann],mean_vec[i] + 8*std_vec[i])))
		test_below_thres = np.ravel(np.nonzero(np.less(data[:,chann],mean_vec[i] - 4*std_vec[i])))
		ind_to_be_corrected = np.append(test_above_thres, test_below_thres)
		ind_to_be_corrected = [ind for ind in ind_to_be_corrected]
		for ind in ind_to_be_corrected:
			if ind + 1 < data.shape[0]:
				data[ind,chann] = (data[ind-1, chann] + data[ind+1,chann])/2.
			else:
				data[ind,chann] = data[ind-1, chann]

	mean_vec = np.mean(data[:,channs], axis = 0)
	std_vec = np.std(data[:,channs], axis = 0)
	trace_dist = 0.25*np.max([std_vec])  # don't include std of time stamps when 

	times = data[:,num_channs-1]

	plt.figure()
	cmap = mpl.cm.brg
	for i, chann in enumerate(channs):
		plt.plot(times[:5000],data[:5000,chann] - mean_vec[i] + i*trace_dist,color=cmap(i/float(len(channs))), label=str(chann))
	plt.xlabel('Time (s)')
	plt.title('LFP Traces')
	plt.legend()

	plt.show()

	return

def get_stim_sync_sig(tdt_tank):
	r = io.TdtIO(dirname = tdt_tank)
	bl = r.read_block(lazy=False,cascade=True)

	for sig in bl.segments[0].analogsignals:
		if (sig.name == 'StWv 1'):
			# Signal is output of stimulator monitor recording voltage on channel stimulation is delivered on
			stim_monitor = np.ravel(sig)
		if (sig.name == 'StWv 2'):
			# Stimulation signal used: this is the base stimulation signal that is turned on/off
			stim_signal = np.ravel(sig)
		if (sig.name == 'StWv 4'):
			# Trigger signal for stimulation. High is ON, Low is OFF.
			stim_on_trig = np.ravel(sig)
		if (sig.name == 'StWv 3'):
			# Control signal for stimulator. When stim_on_trig = 1, this signal equals stim_signal
			stim_delivered = np.ravel(sig)
			stwv_samprate = sig.sampling_rate.item()

	return stim_signal, stim_on_trig, stim_delivered, stwv_samprate


def test_convert_OMNI(data, **kwargs):
	
	#data = pd.read_csv(filename,sep=',',header=None,skiprows=[0,1])
	data = np.array(data)
	time_samps, num_col = data.shape
	crc_flag = np.array(data[:,0])
	ind_crc_pass = [ind for ind in range(0,len(crc_flag)) if crc_flag[ind]==170]

	channel_data = np.zeros([len(ind_crc_pass),num_col-3])  # 3 fewer columns since one is crv flag, one is ramp, and one is time samples
	
	for col in range(0,num_col-3):
		channel_data[:,col] = data[ind_crc_pass,col+1]
	timestamps = data[ind_crc_pass,num_col-1]
	counter_ramp = data[ind_crc_pass,num_col-2]

	corrected_channel_data = channel_data[0,:]
	corrected_counter = [counter_ramp[0]]
	num_cycle = 0
	samps_ind = []

	for i in range(1,len(counter_ramp)):
	#for i in range(1,17000):
		diff = counter_ramp[i] - counter_ramp[i-1]
		diff = int(diff)
		if (i % 1000 == 0):
			print i, diff
		elif (diff > 1):
			print i, diff
		if (diff==1):
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
			#corrected_channel_data = np.vstack([corrected_channel_data,channel_data[i,:]])
		elif (diff == -2**16 +1):
			num_cycle += 1
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
			#corrected_channel_data = np.vstack([corrected_channel_data,channel_data[i,:]])
		else:
			samps_ind.append(i)
			num_samples_insert = diff - 1.
			corrected_counter.extend((counter_ramp[i-1] + range(1,diff+1) + num_cycle*(2**16)).tolist())
			#inter_mat = np.zeros([num_samples_insert+1,num_col-3])
			#for j in range(0,num_col-3):
			#	y = np.interp(range(1,diff),[0, diff], [channel_data[i-1,j], channel_data[i,j]])
			#	y = np.append(y,channel_data[i,j])
			#	inter_mat[:,j] = y
			#corrected_channel_data = np.vstack([corrected_channel_data,inter_mat])
		

	return corrected_counter, corrected_channel_data, samps_ind

def convert_OMNI(data, **kwargs):
	'''
	This method converts csv files saved using the OMNI device to a pandas DataFrame for easy
	analysis in Python.

	Input:
		- filename: string containing the file path for a csv file saved with the OMNI device
	
		
	Output:
		- data: pandas DataFrame, M rows x N columns, M = number of data points, N = number of channels + 1, 
				first N -1 columns corresponds to data from the differnt channels while the Nth column 
				contains the timestamps 

	'''
	#data = pd.read_csv(filename,sep=',',header=None,skiprows=[0,1])
	data = np.array(data)
	time_samps, num_col = data.shape
	crc_flag = np.array(data[:,0])
	ind_crc_pass = [ind for ind in range(0,len(crc_flag)) if crc_flag[ind]==170]

	channel_data = np.zeros([len(ind_crc_pass),num_col-3])  # 3 fewer columns since one is crv flag, one is ramp, and one is time samples
	
	for col in range(0,num_col-3):
		channel_data[:,col] = data[ind_crc_pass,col+1]
	timestamps = data[ind_crc_pass,num_col-1]
	counter_ramp = data[ind_crc_pass,num_col-2]

	corrected_counter = [counter_ramp[0]]
	num_cycle = 0

	# Counter is 16 bit and resets at 2**16. This loop unwraps this cycling so that values are monotonically increasing.
	for i in range(1,len(counter_ramp)):
	#for i in range(1,17000):
		diff = counter_ramp[i] - counter_ramp[i-1]
		diff = int(diff)
		
		if (diff == -2**16 +1):
			num_cycle += 1
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
		else:
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
	
	corrected_counter = np.array(corrected_counter)
	diff_corrected_counter = corrected_counter[1:] - corrected_counter[:-1]
	miss_samp_index = np.ravel(np.nonzero(np.greater(diff_corrected_counter,1)))

	corrected_channel_data = channel_data[0:miss_samp_index[0]+1,:]
	diff = corrected_counter[miss_samp_index[0]+1] - corrected_counter[miss_samp_index[0]]
	diff = int(diff)
	inter_mat = np.zeros([diff,num_col-3])
	for j in range(0,num_col-3):
		y = np.interp(range(1,diff),[0, diff], [channel_data[miss_samp_index[0],j], channel_data[miss_samp_index[0]+1,j]])
		y = np.append(y,channel_data[miss_samp_index[0]+1,j])
		inter_mat[:,j] = y
	corrected_channel_data = np.vstack([corrected_channel_data, inter_mat])
	
	print "There are %i instances of missed samples." % len(miss_samp_index)
	print "Beginning looping through regeneration of data"
	for i in range(1,len(miss_samp_index)):
		print i
		# pad with good data first
		corrected_channel_data = np.vstack([corrected_channel_data, channel_data[miss_samp_index[i-1] + 2:miss_samp_index[i]+1,:]])
		# check number of samples that were skipped and need to be regenerated
		diff = corrected_counter[miss_samp_index[i]+1] - corrected_counter[miss_samp_index[i]]
		diff = int(diff)
		inter_mat = np.zeros([diff,num_col-3])
		# interpolate values to regenerate missing data
		for j in range(0,num_col-3):
			y = np.interp(range(1,diff),[0, diff], [channel_data[miss_samp_index[i],j], channel_data[miss_samp_index[i]+1,j]])
			y = np.append(y,channel_data[miss_samp_index[i]+1,j])
			inter_mat[:,j] = y
		corrected_channel_data = np.vstack([corrected_channel_data,inter_mat])

	if (miss_samp_index[-1]+1 != len(ind_crc_pass)-1):
		print "adding last zeros"
		corrected_channel_data = np.vstack([corrected_channel_data, channel_data[miss_samp_index[-1] + 2:,:]])
		

	return corrected_counter, corrected_channel_data, channel_data, miss_samp_index

def convert_OMNI_from_hdf(hdf_filename, chunk_num, total_chunks):
	'''
	This method converts csv files saved using the OMNI device to a pandas DataFrame for easy
	analysis in Python.

	Input:
		- hdf_filename: string containing the file path for a csv file saved with the OMNI device
		- chunk_num: number of chunk that data is divided into, should be less than the total chunks
		- total_chunks: the number of chunks that data will be divided into
		
	Output:
		- data: pandas DataFrame, M rows x N columns, M = number of data points, N = number of channels + 1, 
				first N -1 columns corresponds to data from the differnt channels while the Nth column 
				contains the timestamps 

	'''
	hdf = tables.openFile(hdf_filename)
	print "Loading data."
	t = time.time()
	data = hdf.root.dataGroup.dataTable[:]['out']
	#data = data[:num_input_samples,:]
	#time_stamps = hdf.root.dataGroup.dataTable[:]['time']
	elapsed = time.time() -t
	print "Loaded data: took %f secs." % (elapsed)
	#time_samps, num_col = data.shape
	crc_flag = np.array(data[:,0])
	
	t = time.time()
	ind_crc_pass = np.ravel(np.nonzero(crc_flag==0))
	elapsed = time.time() -t
	print "Found which inds pass CRC: took %f secs." % (elapsed)
	
	channel_data = data[ind_crc_pass,1:-2]
	print channel_data.shape

	#timestamps = time_stamps[ind_crc_pass]
	counter_ramp = np.array(data[ind_crc_pass,-1], dtype = float) 		# note: if dtype is int16 we can't represent differences in adjacent values of -2**16

	corrected_counter = [counter_ramp[0]]
	num_cycle = 0
	print "Finding missed samples in ramp."
	# Counter is 16 bit and resets at 2**16. This loop unwraps this cycling so that values are monotonically increasing.
	t = time.time()
	for i in range(1,len(counter_ramp)):
		#print float(i)/len(counter_ramp)
		diff = counter_ramp[i] - counter_ramp[i-1]
		diff = int(diff)
		
		if (diff == -2**16 +1):
			num_cycle += 1
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
		else:
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
	print "Corrected counter: took %f secs" % (time.time() - t)
	corrected_counter = np.array(corrected_counter)
	diff_corrected_counter = corrected_counter[1:] - corrected_counter[:-1]
	miss_samp_index = np.ravel(np.nonzero(np.greater(diff_corrected_counter,1)))
	'''
	if chunk_num == 1:
		corrected_channel_data = channel_data[0:miss_samp_index[0]+1,:]
		diff = corrected_counter[miss_samp_index[0]+1] - corrected_counter[miss_samp_index[0]]
		diff = int(diff)
		inter_mat = np.zeros([diff,96])
		for j in range(0,96):
			y = np.interp(range(1,diff),[0, diff], [channel_data[miss_samp_index[0],j], channel_data[miss_samp_index[0]+1,j]])
			y = np.append(y,channel_data[miss_samp_index[0]+1,j])
			inter_mat[:,j] = y
			#print y.shape
		corrected_channel_data = np.vstack([corrected_channel_data, inter_mat])
	else:
		corrected_channel_data = []
	'''
	
	print "There are %i instances of missed samples." % len(miss_samp_index)
	print "Beginning looping through regeneration of data"
	t = time.time()
	for i in range((chunk_num-1)*len(miss_samp_index)/total_chunks, chunk_num*len(miss_samp_index)/total_chunks):
		if (i % 3000 == 0):
			print i/float(len(miss_samp_index)), time.time() - t
		# pad with good data first
		if chunk_num == 1:
			corrected_channel_data = channel_data[0:miss_samp_index[0]+1,:]
		else:
			corrected_channel_data = channel_data[miss_samp_index[i-1] + 2:miss_samp_index[i]+1,:]
		# check numbers of samples that were skipped and need to be regenerated
		diff = corrected_counter[miss_samp_index[i]+1] - corrected_counter[miss_samp_index[i]]
		diff = int(diff)
		inter_mat = np.zeros([diff,96])
		# interpolate values to regenerate missing data
		for j in range(0,96):
			y = np.interp(range(1,diff),[0, diff], [channel_data[miss_samp_index[i],j], channel_data[miss_samp_index[i]+1,j]])
			y = np.append(y,channel_data[miss_samp_index[i]+1,j])
			inter_mat[:,j] = y
		corrected_channel_data = np.vstack([corrected_channel_data,inter_mat])
	print "Regeneration done - took % f secs" % (time.time() - t)
	if (miss_samp_index[-1]+1 != len(ind_crc_pass)-1)&(chunk_num==total_chunks):
		print "Adding last data"
		corrected_channel_data = np.vstack([corrected_channel_data, channel_data[miss_samp_index[-1] + 2:,:]])
	hdf.close()

	num_samples, num_col = corrected_channel_data.shape
	
	t = time.time()
	print "Saving data files."

	# split data into 5 files
	for i in range(1):
		filename = 'Mario20161028-OMNI_b' + str(chunk_num) + '.mat'
		omni = dict()
		omni['corrected_data'] = corrected_channel_data
		sp.io.savemat(filename,omni)
	print "Data saved - took %f secs" % (time.time() - t)
	return corrected_channel_data

def convert_OMNI_basic_stats(hdf_filename):
	'''
	This method converts csv files saved using the OMNI device to a pandas DataFrame for easy
	analysis in Python.

	Input:
		- filename: string containing the file path for a csv file saved with the OMNI device
	
		
	Output:
		- data: pandas DataFrame, M rows x N columns, M = number of data points, N = number of channels + 1, 
				first N -1 columns corresponds to data from the differnt channels while the Nth column 
				contains the timestamps 

	'''
	hdf = tables.openFile(hdf_filename)
	print "Loading data."
	t = time.time()
	data = hdf.root.dataGroup.dataTable[:]['out']
	#data = data[:num_input_samples,:]
	timestamps = hdf.root.dataGroup.dataTable[:]['time']
	dur = timestamps[-1] - timestamps[0]
	expected_samples = int(dur*1000)

	elapsed = time.time() -t
	print "Loaded data: took %f secs." % (elapsed)
	time_samps, num_col = data.shape
	crc_flag = np.array(data[:,0])
	
	t = time.time()
	ind_crc_pass = np.ravel(np.nonzero(crc_flag==0))
	elapsed = time.time() -t
	per = 1 - len(ind_crc_pass)/float(expected_samples)
	print "Found which inds pass CRC: took %f secs." % (elapsed)
	
	channel_data = data[ind_crc_pass,1:-1]

	#timestamps = time_stamps[ind_crc_pass]
	counter_ramp = np.array(data[ind_crc_pass,-1], dtype = float) 		# note: if dtype is int16 we can't represent differences in adjacent values of -2**16

	corrected_counter = [counter_ramp[0]]
	num_cycle = 0
	print "Finding missed samples in ramp."
	# Counter is 16 bit and resets at 2**16. This loop unwraps this cycling so that values are monotonically increasing.
	t = time.time()
	for i in range(1,len(counter_ramp)):
		#print float(i)/len(counter_ramp)
		diff = counter_ramp[i] - counter_ramp[i-1]
		diff = int(diff)
		
		if (diff == -2**16 +1):
			num_cycle += 1
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
		else:
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
	print "Corrected counter: took %f secs" % (time.time() - t)
	corrected_counter = np.array(corrected_counter)
	diff_corrected_counter = corrected_counter[1:] - corrected_counter[:-1]
	miss_samp_index = np.ravel(np.nonzero(np.greater(diff_corrected_counter,1)))

	# check samples that were skipped and need to be regenerated
	diff = corrected_counter[miss_samp_index + 1] - corrected_counter[miss_samp_index]
	total_samps_to_correct = np.sum(diff)
	adjusted_per = total_samps_to_correct/float(expected_samples)
	
	print "PER (pass CRC): %f" % (per)
	print "Adjusted PER: %f" % (adjusted_per)
	print "Duration of recording: %f mins" % (dur/60.)  

	return

def convert_OMNI_from_hdf_singlechannel(data, crc, ramp):
	'''
	This method converts mat files with CRC, data from a single channel, timestamps, and ramps signal. 

	Input:
		- mat_file: string containing the file path for a mat file saved with OMNI device data. mat file should containing a dictionary with keys 'data' (data from single 
			channel), 'timestamps' (timestamps of samples, same length as data signal), 'crc' (parity check for each packet/sample), and 'ramp' (ramp signal, same length as data signal)
		- channel_num: number of the channel that the data comes from, used for file naming at the end
	
	Output:
		- data: pandas DataFrame, M rows x 2 columns, M = number of data points, the first column contains the data samples while the second column 
				contains the timestamps 

	'''
	omni = dict()
	print "Loading data."
	#sp.io.loadmat(mat_file,omni)
	#time_stamps = hdf.root.dataGroup.dataTable[:]['time']
	print "Loaded data."
	#channel_data = [omni['data'], omni['timestamps']]
	channel_data = data
	time_samps = len(data)
	num_col = 1
	#crc_flag = np.array(omni['crc'])
	crc_flag = crc
	print "Finding which "
	ind_crc_pass = np.ravel(np.nonzero(crc_flag==0))
	print "Found which inds pass CRC"
	#channel_data = np.zeros([len(ind_crc_pass),num_col-2])  # 2 fewer columns since one is crv flag and one is ramp

	#counter_ramp = omni['ramp'][ind_crc_pass]
	counter_ramp = ramp

	corrected_counter = [counter_ramp[0]]
	num_cycle = 0
	print "Finding missed samples in ramp."
	# Counter is 16 bit and resets at 2**16. This loop unwraps this cycling so that values are monotonically increasing.
	for i in range(1,len(counter_ramp)):
	#for i in range(1,17000):
		print float(i)/len(counter_ramp)
		diff = counter_ramp[i] - counter_ramp[i-1]
		diff = int(diff)
		
		if (diff == -2**16 +1):
			num_cycle += 1
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
		else:
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
	
	corrected_counter = np.array(corrected_counter)
	diff_corrected_counter = corrected_counter[1:] - corrected_counter[:-1]
	miss_samp_index = np.ravel(np.nonzero(np.greater(diff_corrected_counter,1)))

	corrected_channel_data = channel_data[0:miss_samp_index[0]+1]
	diff = corrected_counter[miss_samp_index[0]+1] - corrected_counter[miss_samp_index[0]]
	diff = int(diff)
	inter_mat = np.zeros(diff)
	
	y = np.interp(range(1,diff),[0, diff], [channel_data[miss_samp_index[0]], channel_data[miss_samp_index[0]+1]])
	y = np.append(y,channel_data[miss_samp_index[0]+1])
	inter_mat = y
	corrected_channel_data = np.append(corrected_channel_data, inter_mat)
	
	print "There are %i instances of missed samples." % len(miss_samp_index)
	print "Beginning looping through regeneration of data"
	for i in range(1,len(miss_samp_index)):
		print float(i)/(len(miss_samp_index)-1)
		# pad with good data first
		corrected_channel_data = np.append(corrected_channel_data, channel_data[miss_samp_index[i-1] + 2:miss_samp_index[i]+1])
		# check number of samples that were skipped and need to be regenerated
		diff = corrected_counter[miss_samp_index[i]+1] - corrected_counter[miss_samp_index[i]]
		diff = int(diff)
		inter_mat = np.zeros(diff)
		# interpolate values to regenerate missing data
		for j in range(0,num_col):
			y = np.interp(range(1,diff),[0, diff], [channel_data[miss_samp_index[i]], channel_data[miss_samp_index[i]+1]])
			y = np.append(y,channel_data[miss_samp_index[i]+1])
			inter_mat = y
		corrected_channel_data = np.append(corrected_channel_data,inter_mat)

	if (miss_samp_index[-1]+1 != len(ind_crc_pass)-1):
		print "adding last zeros"
		corrected_channel_data = np.append(corrected_channel_data, channel_data[miss_samp_index[-1] + 2])
		
	return corrected_counter, corrected_channel_data

def computePowersWithChirplets(channel_data,Avg_Fs,channel,event_indices,t_before, t_after, center_freq):
	'''
	This method extracts spectral amplitudes around a defined center frequency by convolving raw LFP data with Gabor 
	time-frequency basis functions (Gaussian envelope).

	Inputs:
		- channel_data: raw LFP data formatted as a multi-dimensional array of size N_samps x N_channels
		- Avg_Fs: sampling frequency of LFP data
		- channel: channel to perform analysis on, all values should be in range [1,N_channels]
		- event_indices: sample indices that correspond to trial events data is aligned to, one-dimensional of length N_trials 
		- t_before: time before event index to include in analysis, measured in seconds
		- t_after: time after event index to include in analysis, measured in seconds
		- center_freq: center frequency parameter that is used in definition of Gabor atom
	Outputs:
		- trial_powers: power amplitudes formatterd a multi-dimensional array of size N_trials x N_timepoints, where N_timepoints is defined by the window
		                size dictated by the t_before and t_after parameter
		- times: vector of time points for easy plotting after analysis is complete
	'''
	# Define Gabor atom parameters
	v_0 = center_freq
	s_0 = -5.075
	t_0 = 0

	# Define other parameters
	win_before = int(t_before*Avg_Fs)
	win_after = int(t_after*Avg_Fs)
	channel = np.array(channel) - 1 	# adjust so that counting starts at 0
	print "Defining variables"
	times = np.arange(-t_before,t_after,float(t_after + t_before)/(win_after + win_before))
	trial_powers = np.zeros([len(event_indices),2*944])
	windows = np.zeros([len(event_indices),len(times)])

	for i,ind in enumerate(event_indices):
		print i,'/',len(event_indices)
		window = range(int(ind) - win_before,int(ind) + win_after)
		windows[i,:] = window
		#t_0 = -t_before + 1
		gabor_atom = (2**0.25)*np.exp(-0.25*s_0 - np.pi*((times - t_0)**2)*np.exp(-s_0) + 1j*np.pi*(times - t_0)*(2*v_0))
		complex_power = np.convolve(channel_data[window,channel],gabor_atom,mode='full')
		trial_powers[i,:] = np.absolute(complex_power[3*944:5*944]) 	# get power magnitudes
		#trial_powers[i,:] = np.absolute(complex_power) 	# get power magnitudes

	#mlab.specgram(channel_data[window,channel], NFFT=256, Fs=Avg_Fs)
	print "Done looping"
	return trial_powers, times, complex_power, channel_data[window,channel], windows

def powersWithFFT(channel_data,Avg_Fs,channel,event_indices,t_before, t_after):

	win_before = int(t_before*Avg_Fs)
	win_after = int(t_after*Avg_Fs)
	channel = np.array(channel) - 1 	# adjust so that counting starts at 0

	times = np.arange(-t_before,t_after,float(t_after + t_before)/(win_after + win_before))
	window_times = np.arange(-win_before,win_after)
	trial_powers = np.zeros([len(event_indices),len(times)])


	T = 1./Avg_Fs
	N = 256
	x = np.linspace(0.0, N*T, N)
	xf = np.linspace(0.0, 1.0/(2.0*T), N/2)
	for j,ind in enumerate(event_indices[0:10]):
		for i in range(0,len(times)):
			print i,'/',len(times)
			t = np.arange(ind + times[i],ind+256 + times[i],1./Avg_Fs)
			data = channel_data[ind + window_times[i]:ind+256 + window_times[i],channel]/np.sum(channel_data[ind + window_times[i]:ind+256 + window_times[i],channel])
			sp = np.fft.fft(data)
			trial_powers[j,i] = np.absolute(sp[5])**2
	print len(xf)
	print len(sp)
	
	return trial_powers, xf

def powersWithSpecgram(channel_data,Avg_Fs,channel,event_indices,t_before, t_after):

	win_before = int(t_before*Avg_Fs)
	win_after = int(t_after*Avg_Fs)
	channel = np.array(channel) - 1 	# adjust so that counting starts at 0

	times = np.arange(-t_before,t_after,float(t_after + t_before)/(win_after + win_before))
	trial_powers = np.zeros([len(event_indices),28])

	for j,ind in enumerate(event_indices):
		data = channel_data[ind - win_before:ind + win_after,channel]
		data = np.ravel(data)
		Sxx, f, t, fig = specgram(data,Fs=Avg_Fs)
		#Sxx = Sxx/np.sum(Sxx)
		Sxx = 10*np.log10(Sxx)
		trial_powers[j,:] = np.sum(Sxx[3:5,:],axis=0)/2.
		#trial_powers[j,:] = Sxx[5,:]
	return trial_powers, t, f
	#return trial_powers, data

'''
filename_prefix = 'C:/Users/Samantha Summerson/Dropbox/Carmena Lab/OMNI_Device/Data/streams7_20/'
filename = filename_prefix + '20160720-163020.csv'
#filename = filename_prefix + '20160720-171300.csv'
#filename = filename_prefix + '20160720-174338.csv'
data = pd.read_csv(filename,sep=',',header=None,skiprows=[0,1])
print "Data read."
#test_corrected_counter, test_corrected_channel_data, counter = test_convert_OMNI(data)
corrected_counter, corrected_channel_data, channel_data, miss_samp_index = convert_OMNI(data)
'''

def get_OMNI_inds_hold_center(hdf_filename, syncHDF_file, tdt_timepoint, omni_timepoint, omni_freq):
	'''
	Method to get the indices in OMNI recordings of beginning of center hold. 

	Input: 
	- hdf_filename: location of hdf file for the behavior
	- sync_filename: location of sync file
	- tdt_timepoint: a timepoint in tdt recording that can be matched with a timepoint in the omni
					recording, e.g. time of first stimulation pulse
	- omni_timepoint: a timepoint in the OMNI recording that can be matched with a timepoint in the 
					tdt recording
	- omni_freq: sampling frequency of OMNI data, e.g. 1000 = 1 kHz sample rate

	Output:
	- state_row_ind: task timestamps for behavioral events, used to sanity check output
	- omni_inds: sample numbers for OMNI recorded data corresponding to center hold periods
	'''
	
	# Find hdf row numbers corresponding to hold_center times in successful trials only
	table = tables.openFile(hdf_filename)
	state = table.root.task_msgs[:]['msg']
	state_time = table.root.task_msgs[:]['time']
	ind_reward = np.ravel(np.nonzero(state == 'reward'))
	ind_hold_center = ind_reward - 7
	ind_peripheral_on = ind_reward - 6
	ind_go_cue = ind_reward - 3
	ind_hold_peripheral = ind_reward - 2
	ind_end_hold_peripheral = ind_reward - 1
	ind_reward_end = ind_reward + 1

	
	
	# Load syncing data
	hdf_times = dict()
	sp.io.loadmat(syncHDF_file, hdf_times)
	hdf_rows = np.ravel(hdf_times['row_number'])
	hdf_rows = [val for val in hdf_rows]
	dio_tdt_sample = np.ravel(hdf_times['tdt_samplenumber'])
	dio_freq = np.ravel(hdf_times['tdt_dio_samplerate'])

	lfp_dio_sample_num = dio_tdt_sample  # assumes DIOx and LFPx are saved using the same sampling rate

	# Get indices for beginning of hold
	state_row_ind = state_time[ind_hold_center]		# gives the hdf row number sampled at 60 Hz
	state_row_ind_beginhold = state_row_ind
	lfp_state_row_ind = np.zeros(state_row_ind.size)

	for i in range(len(state_row_ind)):
		hdf_index = np.argmin(np.abs(hdf_rows - state_row_ind[i]))
		if np.abs(hdf_rows[hdf_index] - state_row_ind[i])==0:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		elif hdf_rows[hdf_index] > state_row_ind[i]:
			hdf_row_diff = hdf_rows[hdf_index] - hdf_rows[hdf_index -1]  # distance of the interval of the two closest hdf_row_numbers
			m = (lfp_dio_sample_num[hdf_index]-lfp_dio_sample_num[hdf_index - 1])/hdf_row_diff
			b = lfp_dio_sample_num[hdf_index-1] - m*hdf_rows[hdf_index-1]
			lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
		elif (hdf_rows[hdf_index] < state_row_ind[i])&(hdf_index + 1 < len(hdf_rows)):
			hdf_row_diff = hdf_rows[hdf_index + 1] - hdf_rows[hdf_index]
			if (hdf_row_diff > 0):
				m = (lfp_dio_sample_num[hdf_index + 1] - lfp_dio_sample_num[hdf_index])/hdf_row_diff
				b = lfp_dio_sample_num[hdf_index] - m*hdf_rows[hdf_index]
				lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
			else:
				lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		else:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]

	lfp_state_row_ind_beginhold = lfp_state_row_ind

	# Repeat for peripheral target appearing
	state_row_ind = state_time[ind_peripheral_on]
	state_row_ind_peripheralon = state_row_ind
	lfp_state_row_ind = np.zeros(state_row_ind.size)

	for i in range(len(state_row_ind)):
		hdf_index = np.argmin(np.abs(hdf_rows - state_row_ind[i]))
		if np.abs(hdf_rows[hdf_index] - state_row_ind[i])==0:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		elif hdf_rows[hdf_index] > state_row_ind[i]:
			hdf_row_diff = hdf_rows[hdf_index] - hdf_rows[hdf_index -1]  # distance of the interval of the two closest hdf_row_numbers
			m = (lfp_dio_sample_num[hdf_index]-lfp_dio_sample_num[hdf_index - 1])/hdf_row_diff
			b = lfp_dio_sample_num[hdf_index-1] - m*hdf_rows[hdf_index-1]
			lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
		elif (hdf_rows[hdf_index] < state_row_ind[i])&(hdf_index + 1 < len(hdf_rows)):
			hdf_row_diff = hdf_rows[hdf_index + 1] - hdf_rows[hdf_index]
			if (hdf_row_diff > 0):
				m = (lfp_dio_sample_num[hdf_index + 1] - lfp_dio_sample_num[hdf_index])/hdf_row_diff
				b = lfp_dio_sample_num[hdf_index] - m*hdf_rows[hdf_index]
				lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
			else:
				lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		else:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]

	lfp_state_row_ind_peripheralon = lfp_state_row_ind

	# Repeat for end of hold, i.e. GO CUE
	state_row_ind = state_time[ind_go_cue]
	state_row_ind_endhold = state_row_ind
	lfp_state_row_ind = np.zeros(state_row_ind.size)

	for i in range(len(state_row_ind)):
		hdf_index = np.argmin(np.abs(hdf_rows - state_row_ind[i]))
		if np.abs(hdf_rows[hdf_index] - state_row_ind[i])==0:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		elif hdf_rows[hdf_index] > state_row_ind[i]:
			hdf_row_diff = hdf_rows[hdf_index] - hdf_rows[hdf_index -1]  # distance of the interval of the two closest hdf_row_numbers
			m = (lfp_dio_sample_num[hdf_index]-lfp_dio_sample_num[hdf_index - 1])/hdf_row_diff
			b = lfp_dio_sample_num[hdf_index-1] - m*hdf_rows[hdf_index-1]
			lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
		elif (hdf_rows[hdf_index] < state_row_ind[i])&(hdf_index + 1 < len(hdf_rows)):
			hdf_row_diff = hdf_rows[hdf_index + 1] - hdf_rows[hdf_index]
			if (hdf_row_diff > 0):
				m = (lfp_dio_sample_num[hdf_index + 1] - lfp_dio_sample_num[hdf_index])/hdf_row_diff
				b = lfp_dio_sample_num[hdf_index] - m*hdf_rows[hdf_index]
				lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
			else:
				lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		else:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]

	lfp_state_row_ind_endhold = lfp_state_row_ind

	# Repeat for beginning of peripheral hold
	state_row_ind = state_time[ind_hold_peripheral]
	lfp_state_row_ind = np.zeros(state_row_ind.size)

	for i in range(len(state_row_ind)):
		hdf_index = np.argmin(np.abs(hdf_rows - state_row_ind[i]))
		if np.abs(hdf_rows[hdf_index] - state_row_ind[i])==0:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		elif hdf_rows[hdf_index] > state_row_ind[i]:
			hdf_row_diff = hdf_rows[hdf_index] - hdf_rows[hdf_index -1]  # distance of the interval of the two closest hdf_row_numbers
			m = (lfp_dio_sample_num[hdf_index]-lfp_dio_sample_num[hdf_index - 1])/hdf_row_diff
			b = lfp_dio_sample_num[hdf_index-1] - m*hdf_rows[hdf_index-1]
			lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
		elif (hdf_rows[hdf_index] < state_row_ind[i])&(hdf_index + 1 < len(hdf_rows)):
			hdf_row_diff = hdf_rows[hdf_index + 1] - hdf_rows[hdf_index]
			if (hdf_row_diff > 0):
				m = (lfp_dio_sample_num[hdf_index + 1] - lfp_dio_sample_num[hdf_index])/hdf_row_diff
				b = lfp_dio_sample_num[hdf_index] - m*hdf_rows[hdf_index]
				lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
			else:
				lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		else:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]

	lfp_state_row_ind_beginhold_peripheral = lfp_state_row_ind

	# Repeat for end of peripheral hold
	state_row_ind = state_time[ind_end_hold_peripheral]
	state_row_ind_endhold = state_row_ind
	lfp_state_row_ind = np.zeros(state_row_ind.size)

	for i in range(len(state_row_ind)):
		hdf_index = np.argmin(np.abs(hdf_rows - state_row_ind[i]))
		if np.abs(hdf_rows[hdf_index] - state_row_ind[i])==0:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		elif hdf_rows[hdf_index] > state_row_ind[i]:
			hdf_row_diff = hdf_rows[hdf_index] - hdf_rows[hdf_index -1]  # distance of the interval of the two closest hdf_row_numbers
			m = (lfp_dio_sample_num[hdf_index]-lfp_dio_sample_num[hdf_index - 1])/hdf_row_diff
			b = lfp_dio_sample_num[hdf_index-1] - m*hdf_rows[hdf_index-1]
			lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
		elif (hdf_rows[hdf_index] < state_row_ind[i])&(hdf_index + 1 < len(hdf_rows)):
			hdf_row_diff = hdf_rows[hdf_index + 1] - hdf_rows[hdf_index]
			if (hdf_row_diff > 0):
				m = (lfp_dio_sample_num[hdf_index + 1] - lfp_dio_sample_num[hdf_index])/hdf_row_diff
				b = lfp_dio_sample_num[hdf_index] - m*hdf_rows[hdf_index]
				lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
			else:
				lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		else:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]

	lfp_state_row_ind_endhold_peripheral = lfp_state_row_ind

	# Repeat for beginning of reward
	state_row_ind = state_time[ind_reward]
	state_row_ind_endhold = state_row_ind
	lfp_state_row_ind = np.zeros(state_row_ind.size)

	for i in range(len(state_row_ind)):
		hdf_index = np.argmin(np.abs(hdf_rows - state_row_ind[i]))
		if np.abs(hdf_rows[hdf_index] - state_row_ind[i])==0:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		elif hdf_rows[hdf_index] > state_row_ind[i]:
			hdf_row_diff = hdf_rows[hdf_index] - hdf_rows[hdf_index -1]  # distance of the interval of the two closest hdf_row_numbers
			m = (lfp_dio_sample_num[hdf_index]-lfp_dio_sample_num[hdf_index - 1])/hdf_row_diff
			b = lfp_dio_sample_num[hdf_index-1] - m*hdf_rows[hdf_index-1]
			lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
		elif (hdf_rows[hdf_index] < state_row_ind[i])&(hdf_index + 1 < len(hdf_rows)):
			hdf_row_diff = hdf_rows[hdf_index + 1] - hdf_rows[hdf_index]
			if (hdf_row_diff > 0):
				m = (lfp_dio_sample_num[hdf_index + 1] - lfp_dio_sample_num[hdf_index])/hdf_row_diff
				b = lfp_dio_sample_num[hdf_index] - m*hdf_rows[hdf_index]
				lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
			else:
				lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		else:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]

	lfp_state_row_ind_reward = lfp_state_row_ind

	# Repeat for end of reward
	state_row_ind = state_time[ind_reward_end]
	state_row_ind_endhold = state_row_ind
	lfp_state_row_ind = np.zeros(state_row_ind.size)

	for i in range(len(state_row_ind)):
		hdf_index = np.argmin(np.abs(hdf_rows - state_row_ind[i]))
		if np.abs(hdf_rows[hdf_index] - state_row_ind[i])==0:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		elif hdf_rows[hdf_index] > state_row_ind[i]:
			hdf_row_diff = hdf_rows[hdf_index] - hdf_rows[hdf_index -1]  # distance of the interval of the two closest hdf_row_numbers
			m = (lfp_dio_sample_num[hdf_index]-lfp_dio_sample_num[hdf_index - 1])/hdf_row_diff
			b = lfp_dio_sample_num[hdf_index-1] - m*hdf_rows[hdf_index-1]
			lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
		elif (hdf_rows[hdf_index] < state_row_ind[i])&(hdf_index + 1 < len(hdf_rows)):
			hdf_row_diff = hdf_rows[hdf_index + 1] - hdf_rows[hdf_index]
			if (hdf_row_diff > 0):
				m = (lfp_dio_sample_num[hdf_index + 1] - lfp_dio_sample_num[hdf_index])/hdf_row_diff
				b = lfp_dio_sample_num[hdf_index] - m*hdf_rows[hdf_index]
				lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
			else:
				lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		else:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]

	lfp_state_row_ind_reward_end = lfp_state_row_ind

	# lfp_state_row_ind now has the TDT sample numbers corresponding to the behavior indices of interest. these
	# are sampled at the rate dio_freq.

	# Translate lfp_state_row_ind to OMNI sample inds using the two corresponding timepoints. 
	m = float(omni_freq)/dio_freq
	b = omni_timepoint - m*tdt_timepoint

	omni_inds_beginhold = np.rint(m*lfp_state_row_ind_beginhold + b) 	# make sure that indices are integer values
	omni_inds_peripheralon = np.rint(m*lfp_state_row_ind_peripheralon + b)
	omni_inds_endhold = np.rint(m*lfp_state_row_ind_endhold + b)
	omni_inds_peripheralhold = np.rint(m*lfp_state_row_ind_beginhold_peripheral + b)
	omni_inds_endhold_peripheral = np.rint(m*lfp_state_row_ind_endhold_peripheral + b)
	omni_inds_reward = np.rint(m*lfp_state_row_ind_reward + b)
	omni_inds_reward_end = np.rint(m*lfp_state_row_ind_reward_end + b)

	return omni_inds_beginhold, omni_inds_peripheralon,omni_inds_endhold, omni_inds_peripheralhold, omni_inds_endhold_peripheral, omni_inds_reward, omni_inds_reward_end 

def ClosedLoopReactionTimeAnalysis(hdf_filename, task_events_mat_filename, stim_filename, thres, lower_rt, upper_rt, method):
	'''
	This method finds which trials have stimulation during the peripheral hold period of the delayed reach task
	and at what time stimulation occurs relative to the Go Cue.

	Inputs:
	- hdf_filename: string containing hdf filename and location
	- task_events_mat_filename: string containing .mat filename that contains task events indices in terms of OMNI timestamps
	- stim_filename: string containing .mat filename that contains stimulation start and stop times in terms of OMNI timestamps
	- thres: velocity threshold value used for computing reaction time
	- method: 1 or 2
	'''

	# 1. Compute reaction times
	rt, total_vel, skipped_trials = compute_rt_per_trial_CenterOut(hdf_filename, method, thres, False)
	all_rt = np.zeros(len(rt) + len(skipped_trials))
	good_rt = [ind for ind in range(len(all_rt)) if ind not in skipped_trials]
	all_rt[good_rt] = rt

	# Filter trials by reaction time: only consider when rt in the range (0.1, 1) sec
	too_short = np.ravel(np.nonzero(np.less(rt,lower_rt)))
	too_long = np.ravel(np.nonzero(np.greater(rt,upper_rt)))
	bad_trial_inds = np.ravel(np.append(too_short, too_long))
	good_trial_inds = [ind for ind in range(len(all_rt)) if ind not in bad_trial_inds]  	# take only trials where reaction time is in allotted window
	rt_good_trials = all_rt[good_trial_inds]

	# 2. Get OMNI task event times
	task_events = dict()
	sp.io.loadmat(task_events_mat_filename, task_events)
	omni_time_periph_on = np.ravel(task_events['peripheral_on'])					# array of time when peripheral target is shown during center hold
	omni_time_center_hold_begin = np.ravel(task_events['begin_center_hold'])		# array of time when center hold begins
	omni_time_go_cue = np.ravel(task_events['go_cue'])							# array of time when center hold ends
	omni_time_end_reward = np.ravel(task_events['begin_reward']-500)

	time_periph_on = omni_time_periph_on[good_trial_inds]
	time_center_hold_begin = omni_time_center_hold_begin[good_trial_inds]
	time_go_cue = omni_time_go_cue[good_trial_inds]
	time_end_reward = omni_time_end_reward[good_trial_inds]

	# 3. Get stim times
	stims = dict()
	sp.io.loadmat(stim_filename, stims)
	trains = stims['trains']
	stim_start = trains[:,0]
	stim_end = trains[0,1]

	# 4. For each trial, find stim time closest to peripheral on
	time_after_periph = np.zeros(len(good_trial_inds))
	time_before_go_cue = np.zeros(len(good_trial_inds))
	is_during_periph_on = np.zeros(len(good_trial_inds))
	is_during_center_on_alone = np.zeros(len(good_trial_inds))

	ind_during_periph_on = []
	ind_not_during_periph_on = []
	ind_during_center_on = []
	ind_no_stim_trial = []

	for val in range(len(good_trial_inds)):
		# find closest stim
		stim_index = np.argmin(np.abs(stim_start - time_periph_on[val]))
		# find time relative to periph_on: positive is after, negative is before
		time_after_periph[val] = stim_start[stim_index] - time_periph_on[val]
		time_before_go_cue[val] = stim_start[stim_index] - time_go_cue[val]
		is_during_periph_on[val] = (time_after_periph[val] > -0.067)&(time_after_periph[val] < (time_go_cue[val] - time_periph_on[val]))
		is_during_center_on_alone[val] = (time_after_periph[val] < -0.067)&(time_after_periph[val] > (time_center_hold_begin[val] - time_periph_on[val]))

		# build lists of indices for if stim occured when peripheral was shown or not
		if is_during_periph_on[val]:
			ind_during_periph_on.append(val)
		elif is_during_center_on_alone[val]:
			ind_during_center_on.append(val)
		else:
			ind_not_during_periph_on.append(val)

		# did any stim actually happen between the beginning of the hold and end of reward?
		trial_midpoint = (time_center_hold_begin[val] + time_end_reward[val])/2
		stim_index = np.argmin(np.abs(stim_start - trial_midpoint))
		before_center = time_center_hold_begin[val] - stim_start[stim_index]
		after_reward = stim_start[stim_index] - time_end_reward[val]
		if (before_center > 0)&(after_reward > 0):
			ind_no_stim_trial.append(val)

	time_after_periph = time_after_periph/1000. 	# convert from samples to seconds
	time_before_go_cue = time_before_go_cue/1000.

	# 5. Pull out reaction times for different conditions
	rt_stim_during_periph = rt_good_trials[ind_during_periph_on]											# triasl when stim is during peripheral hold
	rt_stim_not_during_periph = rt_good_trials[np.append(ind_not_during_periph_on, ind_during_center_on)] 	# all trials except when stim is on during peripheral hold
	rt_stim_during_center = rt_good_trials[ind_during_center_on]											# trials when stim is during center on, before peripheral is shown
	rt_stim_not_during_hold = rt_good_trials[ind_not_during_periph_on]										# all trials except when stim is during any time in the hold


	return rt_stim_during_periph, rt_stim_not_during_periph, rt_stim_during_center, rt_stim_not_during_hold, time_after_periph, time_before_go_cue,rt_good_trials, ind_no_stim_trial

def ClosedLoopReactionTimeAnalysis_MultipleSessions(hdf_filename, task_events_mat_filename, stim_filename, thres,lower_rt, upper_rt, method):
	'''
	This method finds which trials have stimulation during the peripheral hold period of the delayed reach task
	and at what time stimulation occurs relative to the Go Cue. It takes in data across multiple behavior sessions

	Inputs:
	- hdf_filename: list of strings containing hdf filename and location
	- task_events_mat_filename: list of strings containing .mat filename that contains task events indices in terms of OMNI timestamps
	- stim_filename: list of strings containing .mat filename that contains stimulation start and stop times in terms of OMNI timestamps
	- thres: float value used to set threshold for finding reaction times
	
	To do: (1) add stats
	'''
	num_sessions = len(hdf_filename)
	rt_stim_during_periph = np.array([])
	rt_stim_not_during_periph = np.array([])
	rt_stim_during_center = np.array([])
	rt_stim_not_during_hold = np.array([])
	time_after_periph = np.array([])
	time_before_go_cue = np.array([])
	rt_good_trials = np.array([])
	ind_no_stim_trial = np.array([])

	for k in range(num_sessions):
		output = ClosedLoopReactionTimeAnalysis(hdf_filename[k], task_events_mat_filename[k], stim_filename[k], thres,lower_rt, upper_rt, method)
		rt_stim_during_periph = np.append(rt_stim_during_periph, output[0])
		rt_stim_not_during_periph = np.append(rt_stim_not_during_periph, output[1])
		rt_stim_during_center = np.append(rt_stim_during_center, output[2])
		rt_stim_not_during_hold = np.append(rt_stim_not_during_hold, output[3])
		time_after_periph = np.append(time_after_periph, output[4])
		time_before_go_cue = np.append(time_before_go_cue, output[5])
		rt_good_trials = np.append(rt_good_trials, output[6])
		ind_no_stim_trial = np.append(ind_no_stim_trial, output[7])
		
	print ind_no_stim_trial
	avg_rt_stim_during_periph = np.nanmean(rt_stim_during_periph)
	avg_rt_not_during_periph = np.nanmean(rt_stim_not_during_periph)
	avg_rt_stim_during_center = np.nanmean(rt_stim_during_center)
	avg_rt_stim_not_during_hold = np.nanmean(rt_stim_not_during_hold)

	all_stim_in_hold = np.append(rt_stim_during_periph, rt_stim_during_center)
	avg_rt_stim_during_hold = np.nanmean(all_stim_in_hold)
	sem_rt_stim_during_hold = np.nanstd(all_stim_in_hold)/np.sqrt(len(all_stim_in_hold))

	if not ind_no_stim_trial:
		avg_rt_no_stim_trial = np.nan
		sem_rt_no_stim_trial = np.nan
	else:
		avg_rt_no_stim_trial = np.nanmean(rt_good_trials[ind_no_stim_trial])
		sem_rt_no_stim_trial = np.nanstd(rt_good_trials[ind_no_stim_trial])/np.sqrt(len(ind_no_stim_trial))
	sem_rt_stim_during_periph = np.nanstd(rt_stim_during_periph)/np.sqrt(len(rt_stim_during_periph))
	sem_rt_not_during_periph = np.nanstd(rt_stim_not_during_periph)/np.sqrt(len(rt_stim_not_during_periph))
	sem_rt_stim_during_center = np.nanstd(rt_stim_during_center)/np.sqrt(len(rt_stim_during_center))
	sem_rt_stim_not_during_hold = np.nanstd(rt_stim_not_during_hold)/np.sqrt(len(rt_stim_not_during_hold))
	
	
	# 6. Do statistics
	# t-test
	t_periph_nothold, p_periph_nothold = stats.ttest_ind(np.append(rt_stim_during_periph, rt_stim_during_center), rt_stim_not_during_hold)
	t_periph_notperiph, p_periph_notperiph = stats.ttest_ind(rt_stim_during_periph, rt_stim_not_during_periph)

	# mann-whitney
	stat_periph_nothold, mw_p_periph_nothold = stats.mannwhitneyu(np.append(rt_stim_during_periph, rt_stim_during_center), rt_stim_not_during_hold)
	stat_periph_notperiph, mw_p_periph_notperiph = stats.mannwhitneyu(rt_stim_during_periph, rt_stim_not_during_periph)

	# kruskal-wallis h-test
	h_periph_nothold, kw_p_periph_nothold = stats.kruskal(np.append(rt_stim_during_periph, rt_stim_during_center), rt_stim_not_during_hold)
	h_periph_notperiph, kw_p_periph_notperiph = stats.kruskal(rt_stim_during_periph, rt_stim_not_during_periph)


	print "T-test Results: \n Periph vs Not During Hold - (t, p) = (%f,%f)" % (t_periph_nothold, p_periph_nothold)
	print " Periph vs Not During Periph - (t, p) = (%f,%f)" % (t_periph_notperiph, p_periph_notperiph)
	print "\nMann-Whitney Results:\n Periph vs Not Durhing Hold - (s, p) = (%f,%f)" % (stat_periph_nothold, mw_p_periph_nothold)
	print " Periph vs Not During Periph - (s, p) = (%f,%f)" % (stat_periph_notperiph, mw_p_periph_notperiph)
	print "\nKruskal-Wallis Results: \n Periph vs Not During Hold - (h, p) = (%f,%f)" % (h_periph_nothold, kw_p_periph_nothold)
	print " Periph vs Not During Periph - (h, p) = (%f,%f)" % (h_periph_notperiph, kw_p_periph_notperiph)


	thresname = float(thres*10)
  	lower_rt_name = float(lower_rt*100)
  	upper_rt_name = float(upper_rt*10)
  	plt_name = 'DelayedReach_RT_Method' + str(method) + '_Thres' + str(thresname) + '_lowerRT' + str(lower_rt_name) + 'upperRT' + str(upper_rt_name) + '.png'
  	
  	print plt_name
  	print avg_rt_stim_during_hold - avg_rt_stim_not_during_hold

	# 6. Plot results
	avg_rt = [avg_rt_stim_during_periph, avg_rt_not_during_periph, avg_rt_stim_during_center, avg_rt_stim_not_during_hold, avg_rt_no_stim_trial, avg_rt_stim_during_hold]
	sem_rt = [sem_rt_stim_during_periph, sem_rt_not_during_periph, sem_rt_stim_during_center, sem_rt_stim_not_during_hold, sem_rt_no_stim_trial, sem_rt_stim_during_hold]
	plt.figure(0)
	plt.errorbar(range(6),avg_rt,yerr = sem_rt,fmt = 'o', color = 'k', ecolor = 'k')
	xticklabels = ['Stim during Peripheral', 'Stim not during Peripheral', 'Stim during Center', 'Stim not during hold', 'No Stim']
  	plt.xticks(range(6), xticklabels)
  	plt.xlim((-0.5,5.5))
  	plt.ylabel('Reaction time (s)')
  	plt.title('Average Reaction Time - %s' % (plt_name))
  	plt.show()
	
  	# sort time_after_periph
  	sorted_time_inds = np.argsort(time_after_periph)
  	#time_bins = np.linspace(np.amin(time_after_periph),np.amax(time_after_periph),21)
  	t_min = -0.4
  	t_max = 0.4
  	nbins = 6
  	stepsize = 0.2	# 100 ms
  	#time_bins = np.linspace(t_min,t_max,nbins)		# center hold is 200 ms before peripheral, max peripheral hold is 400 ms
  	time_bins = np.arange(t_min,t_max+stepsize,stepsize)
  	bin_centers = (time_bins[:-1] + time_bins[1:])/2.
  	
	avg_RT_binned = np.zeros(len(bin_centers))
	sem_RT_binned = np.zeros(len(bin_centers))
	RT_binned = dict()

	for j in range(len(bin_centers)):
		rt_inds = np.ravel(np.nonzero(np.greater(time_after_periph, time_bins[j])&np.less(time_after_periph,time_bins[j+1])))	
		RT_binned[j] = rt_good_trials[rt_inds]
		
		avg_RT_binned[j] = np.nanmean(RT_binned[j])
		sem_RT_binned[j] = np.nanstd(RT_binned[j])/np.sqrt(len(RT_binned[j]))

	# Sliding Average
	N = 50
	sorted_rt = rt_good_trials[sorted_time_inds]
	mean_sorted_rt = np.convolve(sorted_rt, np.ones((N,))/N)[N-1:]
	#mean_time_after_periph = np.convolve(time_after_periph[sorted_time_inds], np.ones((N,))/N)[N-1:]

  	plt.figure(1)
  	plt.subplot(1,2,1)
  	plt.scatter(time_after_periph, rt_good_trials, c = 'c')
  	plt.errorbar(bin_centers, avg_RT_binned, yerr = sem_RT_binned, color = 'k', ecolor = 'k')
  	plt.plot(time_after_periph[sorted_time_inds], mean_sorted_rt, 'm')
  	plt.xlabel('Time relative to Peripheral On (s)')
  	plt.xlim((t_min*1.05,t_max*1.05))
  	plt.ylim((lower_rt,upper_rt))
  	plt.ylabel('Reaction time (s)')
  	

  	# sort time_before_go_cue
  	sorted_time_inds_before_gocue = np.argsort(time_before_go_cue)
  	#time_bins = np.linspace(np.amin(time_after_periph),np.amax(time_after_periph),21)
  	t_min = -0.6
  	t_max = 0.4
  	nbins = 6
  	stepsize = 0.2		# 100 ms
  	#time_bins = np.linspace(t_min,t_max,nbins)		# center hold is 200 ms before peripheral, max peripheral hold is 400 ms
  	time_bins = np.arange(t_min,t_max+stepsize,stepsize)
  	bin_centers_before_gocue = (time_bins[:-1] + time_bins[1:])/2.
	avg_RT_binned_before_gocue = np.zeros(len(bin_centers_before_gocue))
	sem_RT_binned_before_gocue = np.zeros(len(bin_centers_before_gocue))
	RT_binned_before_gocue = dict()

	for j in range(len(bin_centers_before_gocue)):
		rt_inds = np.ravel(np.nonzero(np.greater(time_before_go_cue, time_bins[j])&np.less(time_before_go_cue,time_bins[j+1])))	
		RT_binned_before_gocue[j] = rt_good_trials[rt_inds]

		avg_RT_binned_before_gocue[j] = np.nanmean(RT_binned_before_gocue[j])
		sem_RT_binned_before_gocue[j] = np.nanstd(RT_binned_before_gocue[j])/np.sqrt(len(RT_binned_before_gocue[j]))

	# Sliding Average
	N = 50
	sorted_rt_before_gocue = rt_good_trials[sorted_time_inds_before_gocue]
	mean_sorted_rt_before_gocue = np.convolve(sorted_rt_before_gocue, np.ones((N,))/N)[N-1:]
	#mean_time_after_periph = np.convolve(time_after_periph[sorted_time_inds], np.ones((N,))/N)[N-1:]

  	plt.figure(1)
  	plt.subplot(1,2,2)
  	plt.scatter(time_before_go_cue, rt_good_trials, c = 'c')
  	plt.errorbar(bin_centers_before_gocue, avg_RT_binned_before_gocue, yerr = sem_RT_binned_before_gocue, color = 'k', ecolor = 'k')
  	plt.plot(time_before_go_cue[sorted_time_inds_before_gocue], mean_sorted_rt_before_gocue, 'm')
  	plt.xlabel('Time relative to Go Cue (s)')
  	plt.xlim((t_min*1.05,t_max*1.05))
  	plt.ylim((lower_rt,upper_rt))
  	plt.ylabel('Reaction time (s)')
  	#plt.show()

  	
  	thresname = float(thres*10)
  	lower_rt_name = float(lower_rt*100)
  	upper_rt_name = float(upper_rt*10)
  	plt_name = 'DelayedReach_RT_Method' + str(method) + '_Thres' + str(thresname) + '_lowerRT' + str(lower_rt_name) + 'upperRT' + str(upper_rt_name) + '.png'
  	plt.savefig(plt_name)
  	plt.close()
  	'''
  	# Plot RT distributions: stim during peripheral vs stim not during peripheral
  	# First, fit with log-normal distribution
  	x_stim_during_periph, pdf_stim_during_periph, cdf_stim_during_periph = rt_lognormal_fit(rt_stim_during_periph)
  	x_stim_not_during_periph, pdf_stim_not_during_periphd, cdf_stim_not_during_periph = rt_lognormal_fit(rt_stim_not_during_periph)
	x_stim_not_during_hold, pdf_stim_not_during_hold, cdf_stim_not_during_hold = rt_lognormal_fit(rt_stim_not_during_hold)


  	plt.figure(4)
  	plt.hist(rt_stim_during_periph,bins=10,normed=True,color = 'c',alpha = 0.75)
  	plt.plot(x_stim_during_periph,pdf_stim_during_periph,'k')
  	plt.title('PDF of Reaction Times for Stim During Peripheral')
  	plt.xlabel('Reaction Time (s)')
  	plt.ylabel('Probability')


  	rt_stim_during_periph_sorted_inds = np.argsort(rt_stim_during_periph)
  	rt_stim_not_during_periph_sorted_inds = np.argsort(rt_stim_not_during_periph)
  	rt_stim_not_during_hold_sorted_inds = np.argsort(rt_stim_not_during_hold)


  	plt.figure(3)
  	plt.plot(rt_stim_during_periph[rt_stim_during_periph_sorted_inds],np.arange(len(rt_stim_during_periph))/float(len(rt_stim_during_periph)), 'c',label = 'During Peripheral')
  	plt.plot(x_stim_during_periph,cdf_stim_during_periph,'k',label='During Peripheral Fit')
  	plt.plot(rt_stim_not_during_periph[rt_stim_not_during_periph_sorted_inds],np.arange(len(rt_stim_not_during_periph))/float(len(rt_stim_not_during_periph)), 'b',label = 'Not During Peripheral')
  	plt.plot(x_stim_not_during_periph,cdf_stim_not_during_periph,'m',label='Not During Peripheral Fit')
  	plt.plot(rt_stim_not_during_hold[rt_stim_not_during_hold_sorted_inds],np.arange(len(rt_stim_not_during_hold))/float(len(rt_stim_not_during_hold)), 'y',label = 'Not During Hold')
  	plt.plot(x_stim_not_during_hold,cdf_stim_not_during_hold,'g',label='Not During Hold Fit')
  	
  	plt.xlabel('Reaction Time (s)')
  	plt.ylabel('Fraction of Trials')
  	plt.legend()
  	plt.ylim((-0.05,1.05))
  	plt.show()
	'''
  	#### add: find trials completely without stimulation to compare with
	return RT_binned, bin_centers, RT_binned_before_gocue, bin_centers_before_gocue

def rt_lognormal_fit(rt):
	s, loc, scale = stats.lognorm.fit(rt, floc = 0)
	xmin = rt.min()
  	xmax = rt.max()
  	x = np.linspace(xmin,xmax,100)
  	pdf = stats.lognorm.pdf(x,s,scale = scale)
  	cdf = np.cumsum(pdf)/np.sum(pdf)

	return x, pdf, cdf
