import numpy as np
import scipy as sp
from scipy import stats
import statsmodels.api as sm
from scipy import io
from scipy import signal
import re
#from neo import io
import pandas as pd
from matplotlib import pyplot as plt
import matplotlib as mpl
from matplotlib import mlab
import tables
from pylab import specgram
import time
from rt_calc import compute_rt_per_trial_CenterOut
from spectralAnalysis import averagedPSD
from neo import io
import tables




def plotRawLFPTraces(data, **kwargs):
	'''
	This method plots the raw LFP data for all channels or a subset of channels in a single plot 
	for easy viewing. Data is normalized to have zero DC component. Spacing between traces is determined by 
	the max standard deviation across all channels.

	Input:
		- data: numpy array, as generated by the convert_OMNI method
	Optional input:
		- channs: list of channels to be plotted
		- filter_data: True/False if low-pass filter is applied to data, default is False
	'''
	num_channs = data.shape[1]
	 		
	if kwargs:
		channs = kwargs['channs']
	else:
		channs = range(1,num_channs+1,1) 	# recall one column is for timestamps, not channel data

	channs = [(chann - 1) for chann in channs]  # channel numbers are offset by 1 from index values

	'''
	if filter_data:
		cutoff_f = 100
		cutoff_f = cutoff_f/(1000./2)  # sampling rate is 1000 Hz
		num_taps = 100
		lpf = signal.firwin(num_taps,cutoff_f,window='hamming')
		for chann in channs:
			data[:][chann] = signal.lfilter(lpf,1,data[:][chann])
	'''

	mean_vec = np.mean(data[:,channs], axis = 0)
	std_vec = np.std(data[:,channs], axis = 0)

	for i, chann in enumerate(channs):
		test_above_thres = np.ravel(np.nonzero(np.greater(data[:,chann],mean_vec[i] + 8*std_vec[i])))
		test_below_thres = np.ravel(np.nonzero(np.less(data[:,chann],mean_vec[i] - 4*std_vec[i])))
		ind_to_be_corrected = np.append(test_above_thres, test_below_thres)
		ind_to_be_corrected = [ind for ind in ind_to_be_corrected]
		for ind in ind_to_be_corrected:
			if ind + 1 < data.shape[0]:
				data[ind,chann] = (data[ind-1, chann] + data[ind+1,chann])/2.
			else:
				data[ind,chann] = data[ind-1, chann]

	mean_vec = np.mean(data[:,channs], axis = 0)
	std_vec = np.std(data[:,channs], axis = 0)
	trace_dist = 0.25*np.max([std_vec])  # don't include std of time stamps when 

	times = data[:,num_channs-1]

	plt.figure()
	cmap = mpl.cm.brg
	for i, chann in enumerate(channs):
		plt.plot(times[:5000],data[:5000,chann] - mean_vec[i] + i*trace_dist,color=cmap(i/float(len(channs))), label=str(chann))
	plt.xlabel('Time (s)')
	plt.title('LFP Traces')
	plt.legend()

	plt.show()

	return

def get_stim_sync_sig(tdt_tank):
	r = io.TdtIO(dirname = tdt_tank)
	bl = r.read_block(lazy=False,cascade=True)

	for sig in bl.segments[0].analogsignals:
		if (sig.name == 'StWv 1'):
			# Signal is output of stimulator monitor recording voltage on channel stimulation is delivered on
			stim_monitor = np.ravel(sig)
		if (sig.name == 'StWv 2'):
			# Stimulation signal used: this is the base stimulation signal that is turned on/off
			stim_signal = np.ravel(sig)
		if (sig.name == 'StWv 4'):
			# Trigger signal for stimulation. High is ON, Low is OFF.
			stim_on_trig = np.ravel(sig)
		if (sig.name == 'StWv 3'):
			# Control signal for stimulator. When stim_on_trig = 1, this signal equals stim_signal
			stim_delivered = np.ravel(sig)
			stwv_samprate = sig.sampling_rate.item()

	return stim_signal, stim_on_trig, stim_delivered, stwv_samprate


def test_convert_OMNI(data, **kwargs):
	
	#data = pd.read_csv(filename,sep=',',header=None,skiprows=[0,1])
	data = np.array(data)
	time_samps, num_col = data.shape
	crc_flag = np.array(data[:,0])
	ind_crc_pass = [ind for ind in range(0,len(crc_flag)) if crc_flag[ind]==170]

	channel_data = np.zeros([len(ind_crc_pass),num_col-3])  # 3 fewer columns since one is crv flag, one is ramp, and one is time samples
	
	for col in range(0,num_col-3):
		channel_data[:,col] = data[ind_crc_pass,col+1]
	timestamps = data[ind_crc_pass,num_col-1]
	counter_ramp = data[ind_crc_pass,num_col-2]

	corrected_channel_data = channel_data[0,:]
	corrected_counter = [counter_ramp[0]]
	num_cycle = 0
	samps_ind = []

	for i in range(1,len(counter_ramp)):
	#for i in range(1,17000):
		diff = counter_ramp[i] - counter_ramp[i-1]
		diff = int(diff)
		if (i % 1000 == 0):
			print i, diff
		elif (diff > 1):
			print i, diff
		if (diff==1):
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
			#corrected_channel_data = np.vstack([corrected_channel_data,channel_data[i,:]])
		elif (diff == -2**16 +1):
			num_cycle += 1
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
			#corrected_channel_data = np.vstack([corrected_channel_data,channel_data[i,:]])
		else:
			samps_ind.append(i)
			num_samples_insert = diff - 1.
			corrected_counter.extend((counter_ramp[i-1] + range(1,diff+1) + num_cycle*(2**16)).tolist())
			#inter_mat = np.zeros([num_samples_insert+1,num_col-3])
			#for j in range(0,num_col-3):
			#	y = np.interp(range(1,diff),[0, diff], [channel_data[i-1,j], channel_data[i,j]])
			#	y = np.append(y,channel_data[i,j])
			#	inter_mat[:,j] = y
			#corrected_channel_data = np.vstack([corrected_channel_data,inter_mat])
		

	return corrected_counter, corrected_channel_data, samps_ind

def convert_OMNI(data, **kwargs):
	'''
	This method converts csv files saved using the OMNI device to a pandas DataFrame for easy
	analysis in Python.

	Input:
		- filename: string containing the file path for a csv file saved with the OMNI device
	
		
	Output:
		- data: pandas DataFrame, M rows x N columns, M = number of data points, N = number of channels + 1, 
				first N -1 columns corresponds to data from the differnt channels while the Nth column 
				contains the timestamps 

	'''
	#data = pd.read_csv(filename,sep=',',header=None,skiprows=[0,1])
	data = np.array(data)
	time_samps, num_col = data.shape
	crc_flag = np.array(data[:,0])
	ind_crc_pass = [ind for ind in range(0,len(crc_flag)) if crc_flag[ind]==170]

	channel_data = np.zeros([len(ind_crc_pass),num_col-3])  # 3 fewer columns since one is crv flag, one is ramp, and one is time samples
	
	for col in range(0,num_col-3):
		channel_data[:,col] = data[ind_crc_pass,col+1]
	timestamps = data[ind_crc_pass,num_col-1]
	counter_ramp = data[ind_crc_pass,num_col-2]

	corrected_counter = [counter_ramp[0]]
	num_cycle = 0

	# Counter is 16 bit and resets at 2**16. This loop unwraps this cycling so that values are monotonically increasing.
	for i in range(1,len(counter_ramp)):
	#for i in range(1,17000):
		diff = counter_ramp[i] - counter_ramp[i-1]
		diff = int(diff)
		
		if (diff == -2**16 +1):
			num_cycle += 1
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
		else:
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
	
	corrected_counter = np.array(corrected_counter)
	diff_corrected_counter = corrected_counter[1:] - corrected_counter[:-1]
	miss_samp_index = np.ravel(np.nonzero(np.greater(diff_corrected_counter,1)))

	corrected_channel_data = channel_data[0:miss_samp_index[0]+1,:]
	diff = corrected_counter[miss_samp_index[0]+1] - corrected_counter[miss_samp_index[0]]
	diff = int(diff)
	inter_mat = np.zeros([diff,num_col-3])
	for j in range(0,num_col-3):
		y = np.interp(range(1,diff),[0, diff], [channel_data[miss_samp_index[0],j], channel_data[miss_samp_index[0]+1,j]])
		y = np.append(y,channel_data[miss_samp_index[0]+1,j])
		inter_mat[:,j] = y
	corrected_channel_data = np.vstack([corrected_channel_data, inter_mat])
	
	print "There are %i instances of missed samples." % len(miss_samp_index)
	print "Beginning looping through regeneration of data"
	for i in range(1,len(miss_samp_index)):
		print i
		# pad with good data first
		corrected_channel_data = np.vstack([corrected_channel_data, channel_data[miss_samp_index[i-1] + 2:miss_samp_index[i]+1,:]])
		# check number of samples that were skipped and need to be regenerated
		diff = corrected_counter[miss_samp_index[i]+1] - corrected_counter[miss_samp_index[i]]
		diff = int(diff)
		inter_mat = np.zeros([diff,num_col-3])
		# interpolate values to regenerate missing data
		for j in range(0,num_col-3):
			y = np.interp(range(1,diff),[0, diff], [channel_data[miss_samp_index[i],j], channel_data[miss_samp_index[i]+1,j]])
			y = np.append(y,channel_data[miss_samp_index[i]+1,j])
			inter_mat[:,j] = y
		corrected_channel_data = np.vstack([corrected_channel_data,inter_mat])

	if (miss_samp_index[-1]+1 != len(ind_crc_pass)-1):
		print "adding last zeros"
		corrected_channel_data = np.vstack([corrected_channel_data, channel_data[miss_samp_index[-1] + 2:,:]])
		

	return corrected_counter, corrected_channel_data, channel_data, miss_samp_index

def convert_OMNI_from_hdf(hdf_filename, chunk_num, total_chunks):
	'''
	This method converts csv files saved using the OMNI device to a pandas DataFrame for easy
	analysis in Python.

	Input:
		- hdf_filename: string containing the file path for a csv file saved with the OMNI device
		- chunk_num: number of chunk that data is divided into, should be less than the total chunks
		- total_chunks: the number of chunks that data will be divided into
		
	Output:
		- data: pandas DataFrame, M rows x N columns, M = number of data points, N = number of channels + 1, 
				first N -1 columns corresponds to data from the differnt channels while the Nth column 
				contains the timestamps 

	'''
	hdf = tables.openFile(hdf_filename)
	print "Loading data."
	t = time.time()
	data = hdf.root.dataGroup.dataTable[:]['out']
	#data = data[:num_input_samples,:]
	#time_stamps = hdf.root.dataGroup.dataTable[:]['time']
	elapsed = time.time() -t
	print "Loaded data: took %f secs." % (elapsed)
	#time_samps, num_col = data.shape
	crc_flag = np.array(data[:,0])
	
	t = time.time()
	ind_crc_pass = np.ravel(np.nonzero(crc_flag==0))
	elapsed = time.time() -t
	print "Found which inds pass CRC: took %f secs." % (elapsed)
	
	channel_data = data[ind_crc_pass,1:-2]
	print channel_data.shape

	#timestamps = time_stamps[ind_crc_pass]
	counter_ramp = np.array(data[ind_crc_pass,-1], dtype = float) 		# note: if dtype is int16 we can't represent differences in adjacent values of -2**16

	corrected_counter = [counter_ramp[0]]
	num_cycle = 0
	print "Finding missed samples in ramp."
	# Counter is 16 bit and resets at 2**16. This loop unwraps this cycling so that values are monotonically increasing.
	t = time.time()
	for i in range(1,len(counter_ramp)):
		#print float(i)/len(counter_ramp)
		diff = counter_ramp[i] - counter_ramp[i-1]
		diff = int(diff)
		
		if (diff == -2**16 +1):
			num_cycle += 1
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
		else:
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
	print "Corrected counter: took %f secs" % (time.time() - t)
	corrected_counter = np.array(corrected_counter)
	diff_corrected_counter = corrected_counter[1:] - corrected_counter[:-1]
	miss_samp_index = np.ravel(np.nonzero(np.greater(diff_corrected_counter,1)))
	'''
	if chunk_num == 1:
		corrected_channel_data = channel_data[0:miss_samp_index[0]+1,:]
		diff = corrected_counter[miss_samp_index[0]+1] - corrected_counter[miss_samp_index[0]]
		diff = int(diff)
		inter_mat = np.zeros([diff,96])
		for j in range(0,96):
			y = np.interp(range(1,diff),[0, diff], [channel_data[miss_samp_index[0],j], channel_data[miss_samp_index[0]+1,j]])
			y = np.append(y,channel_data[miss_samp_index[0]+1,j])
			inter_mat[:,j] = y
			#print y.shape
		corrected_channel_data = np.vstack([corrected_channel_data, inter_mat])
	else:
		corrected_channel_data = []
	'''
	
	print "There are %i instances of missed samples." % len(miss_samp_index)
	print "Beginning looping through regeneration of data"
	t = time.time()
	for i in range((chunk_num-1)*len(miss_samp_index)/total_chunks, chunk_num*len(miss_samp_index)/total_chunks):
		if (i % 3000 == 0):
			print i/float(len(miss_samp_index)), time.time() - t
		# pad with good data first
		if chunk_num == 1:
			corrected_channel_data = channel_data[0:miss_samp_index[0]+1,:]
		else:
			corrected_channel_data = channel_data[miss_samp_index[i-1] + 2:miss_samp_index[i]+1,:]
		# check numbers of samples that were skipped and need to be regenerated
		diff = corrected_counter[miss_samp_index[i]+1] - corrected_counter[miss_samp_index[i]]
		diff = int(diff)
		inter_mat = np.zeros([diff,96])
		# interpolate values to regenerate missing data
		for j in range(0,96):
			y = np.interp(range(1,diff),[0, diff], [channel_data[miss_samp_index[i],j], channel_data[miss_samp_index[i]+1,j]])
			y = np.append(y,channel_data[miss_samp_index[i]+1,j])
			inter_mat[:,j] = y
		corrected_channel_data = np.vstack([corrected_channel_data,inter_mat])
	print "Regeneration done - took % f secs" % (time.time() - t)
	if (miss_samp_index[-1]+1 != len(ind_crc_pass)-1)&(chunk_num==total_chunks):
		print "Adding last data"
		corrected_channel_data = np.vstack([corrected_channel_data, channel_data[miss_samp_index[-1] + 2:,:]])
	hdf.close()

	num_samples, num_col = corrected_channel_data.shape
	
	t = time.time()
	print "Saving data files."

	# split data into 5 files
	for i in range(1):
		filename = 'Mario20161026-OMNI_b' + str(chunk_num) + '.mat'
		omni = dict()
		omni['corrected_data'] = corrected_channel_data
		sp.io.savemat(filename,omni)
	print "Data saved - took %f secs" % (time.time() - t)
	return corrected_channel_data

def convert_OMNI_from_hdf_overnight(hdf_filename, total_chunks):
	'''
	This method converts csv files saved using the OMNI device to a pandas DataFrame for easy
	analysis in Python.

	Input:
		- hdf_filename: string containing the file path for a csv file saved with the OMNI device
		- total_chunks: the number of chunks that data will be divided into

	'''
	hdf = tables.openFile(hdf_filename)
	print "Loading data."
	t = time.time()
	data = hdf.root.dataGroup.dataTable[:]['out']
	#data = data[:num_input_samples,:]
	#timestamps = hdf.root.dataGroup.dataTable[:]['time']
	#dur = timestamps[-1] - timestamps[0]

	elapsed = time.time() -t
	print "Loaded data: took %f secs." % (elapsed)
	#time_samps, num_col = data.shape
	crc_flag = np.array(data[:,0])
	
	t = time.time()
	ind_crc_pass = np.ravel(np.nonzero(crc_flag==0))
	elapsed = time.time() -t
	print "Found which inds pass CRC: took %f secs." % (elapsed)
	
	channel_data = data[ind_crc_pass,1:-2]
	print channel_data.shape
	num_time_samps, num_col = channel_data.shape
	num_col = int(num_col)

	#timestamps = time_stamps[ind_crc_pass]
	counter_ramp = np.array(data[ind_crc_pass,-1], dtype = float) 		# note: if dtype is int16 we can't represent differences in adjacent values of -2**16

	corrected_counter = [counter_ramp[0]]
	num_cycle = 0
	print "Finding missed samples in ramp."
	# Counter is 16 bit and resets at 2**16. This loop unwraps this cycling so that values are monotonically increasing.
	t = time.time()
	for i in range(1,len(counter_ramp)):
		#print float(i)/len(counter_ramp)
		diff = counter_ramp[i] - counter_ramp[i-1]
		diff = int(diff)
		
		if (diff == -2**16 +1):
			num_cycle += 1
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
		else:
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
	print "Corrected counter: took %f secs" % (time.time() - t)

	# corrected counter should be same length at final corrected recordings
	
	num_corrected_samples = len(corrected_counter)
	num_samps_per_chunk = int(num_corrected_samples/total_chunks)

	corrected_counter = np.array(corrected_counter)
	diff_corrected_counter = corrected_counter[1:] - corrected_counter[:-1]
	miss_samp_index = np.ravel(np.nonzero(np.greater(diff_corrected_counter,1)))
	
	corrected_channel_data = channel_data[0:miss_samp_index[0]+1]
	diff = corrected_counter[miss_samp_index[0]+1] - corrected_counter[miss_samp_index[0]]
	diff = int(diff)
	inter_mat = np.zeros([diff,num_col])
	
	for j in range(0,num_col):
		y = np.interp(range(1,diff),[0, diff], [channel_data[miss_samp_index[0],j], channel_data[miss_samp_index[0]+1,j]])
		y = np.append(y,channel_data[miss_samp_index[0]+1,j])
		inter_mat[:,j] = y
	corrected_channel_data = np.vstack([corrected_channel_data,inter_mat])
	
	print "There are %i instances of missed samples." % len(miss_samp_index)
	print "Beginning looping through regeneration of data"
	t = time.time()
	chunk_counter = 0		# variable to count number of chunks recording is split into
	chunk_end = 0			# variable to keep track of when last chunk stopped
	for i in range(1,len(miss_samp_index)):
		if (i % 3000 == 0):
			print i/float(len(miss_samp_index)), i, corrected_channel_data.shape, time.time() - t
		
		corrected_channel_data = np.vstack([corrected_channel_data, channel_data[miss_samp_index[i-1] + 2:miss_samp_index[i]+1,:]])
		# check numbers of samples that were skipped and need to be regenerated
		diff = corrected_counter[miss_samp_index[i]+1] - corrected_counter[miss_samp_index[i]]
		diff = int(diff)
		inter_mat = np.zeros([diff,num_col])
		# interpolate values to regenerate missing data
		for j in range(0,num_col):
			y = np.interp(range(1,diff),[0, diff], [channel_data[miss_samp_index[i],j], channel_data[miss_samp_index[i]+1,j]])
			y = np.append(y,channel_data[miss_samp_index[i]+1,j])
			inter_mat[:,j] = y
		corrected_channel_data = np.vstack([corrected_channel_data,inter_mat])
		
		if (corrected_channel_data.shape[0] > (chunk_counter+1)*num_samps_per_chunk):
			filename = 'Mario20161026-OMNI_b' + str(chunk_counter) + '.mat'
			omni = dict()
			omni['corrected_data'] = corrected_channel_data[chunk_end:-1]
			sp.io.savemat(filename,omni)
			print "Data saved - took %f secs" % (time.time() - t)
			chunk_counter += 1
			chunk_end = corrected_channel_data.shape[0]

	print "Regeneration done - took % f secs" % (time.time() - t)
	if (miss_samp_index[-1]+1 != len(ind_crc_pass)-1)&(chunk_counter<=total_chunks):
		print "Adding last data"
		corrected_channel_data = np.vstack([corrected_channel_data, channel_data[miss_samp_index[-1] + 2:,:]])
		filename = 'Mario20161026-OMNI_b' + str(chunk_counter) + '.mat'
		omni = dict()
		omni['corrected_data'] = corrected_channel_data[chunk_end:-1]
		sp.io.savemat(filename,omni)
		print "Data saved - took %f secs" % (time.time() - t)
	hdf.close()

	return 

def OMNI_detect_sleep_spindles(data, amp_threshold, channel):
	'''
	This method detects when sleep spindles occur in a matrix of data recorded with the OMNI device. It is assumed
	that the data has already been corrected and that the sampling rate is 1000 Hz. Sleep spindles are detected
	by first band-pass filtering the LFP activity into the spindle band (11 - 16 Hz), thresholding the resulting
	signal amplitude, and ensuring that the activity lasts at least 0.5 s (500 ms).

	Inputs:
	- data: M x N array, where M is the number of time samples and N is the number of channels
	- amp_threshold: float representing the value in uV used to threshold the filtered data when detecting the
		occurance of sleep spindles
	- channel: integer in range 1 to N

	Outputs:
	- time_sleep: length P arrays, where P is the number of times sleep spindles were detected, and each element of the
		array represents the time index at which the sleep spindle began
	'''
	Fs = 1000.
	nyq = 0.5*Fs
	order = 5
	Wn_start = 11. / nyq
	Wn_stop = 16. / nyq
	chan = channel - 1

	# 1. Filter data in band [11,16] Hz
	b, a = signal.butter(order, [Wn_start, Wn_stop], btype= 'bandpass', analog = False)
	filtered_data = signal.filtfilt(b,a,data[:,chan])

	# 2. Apply amplitude threshold.
	filtered_data = filtered_data - np.nanmean(filtered_data) 	# make sure the data has zero mean
	above_thres = np.greater(filtered_data,amp_threshold)		# check when data is above threshold
	above_thres_ind = np.ravel(np.nonzero(above_thres))			# find indicated when data is above threshold

	# 3. Check for epochs of activity being above threshold
	neighboring_inds = above_thres_ind[1:] - above_thres_ind[:-1]
	epoch_inds = np.greater(neighboring_inds,)

	return

def convert_OMNI_basic_stats(hdf_filename):
	'''
	This method converts csv files saved using the OMNI device to a pandas DataFrame for easy
	analysis in Python.

	Input:
		- filename: string containing the file path for a csv file saved with the OMNI device
	
		
	Output:
		- data: pandas DataFrame, M rows x N columns, M = number of data points, N = number of channels + 1, 
				first N -1 columns corresponds to data from the differnt channels while the Nth column 
				contains the timestamps 

	'''
	hdf = tables.openFile(hdf_filename)
	print "Loading data."
	t = time.time()
	data = hdf.root.dataGroup.dataTable[:]['out']
	#data = data[:num_input_samples,:]
	timestamps = hdf.root.dataGroup.dataTable[:]['time']
	dur = timestamps[-1] - timestamps[0]
	expected_samples = int(dur*1000)

	elapsed = time.time() -t
	print "Loaded data: took %f secs." % (elapsed)
	time_samps, num_col = data.shape
	crc_flag = np.array(data[:,0])
	
	t = time.time()
	ind_crc_pass = np.ravel(np.nonzero(crc_flag==0))
	elapsed = time.time() -t
	per = 1 - len(ind_crc_pass)/float(expected_samples)
	print "Found which inds pass CRC: took %f secs." % (elapsed)
	
	channel_data = data[ind_crc_pass,1:-1]

	#timestamps = time_stamps[ind_crc_pass]
	counter_ramp = np.array(data[ind_crc_pass,-1], dtype = float) 		# note: if dtype is int16 we can't represent differences in adjacent values of -2**16

	corrected_counter = [counter_ramp[0]]
	num_cycle = 0
	print "Finding missed samples in ramp."
	# Counter is 16 bit and resets at 2**16. This loop unwraps this cycling so that values are monotonically increasing.
	t = time.time()
	for i in range(1,len(counter_ramp)):
		#print float(i)/len(counter_ramp)
		diff = counter_ramp[i] - counter_ramp[i-1]
		diff = int(diff)
		
		if (diff == -2**16 +1):
			num_cycle += 1
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
		else:
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
	print "Corrected counter: took %f secs" % (time.time() - t)
	corrected_counter = np.array(corrected_counter)
	diff_corrected_counter = corrected_counter[1:] - corrected_counter[:-1]
	miss_samp_index = np.ravel(np.nonzero(np.greater(diff_corrected_counter,1)))

	# check samples that were skipped and need to be regenerated
	diff = corrected_counter[miss_samp_index + 1] - corrected_counter[miss_samp_index]
	total_samps_to_correct = np.sum(diff)
	adjusted_per = total_samps_to_correct/float(expected_samples)
	
	print "PER (pass CRC): %f" % (per)
	print "Adjusted PER: %f" % (adjusted_per)
	print "Duration of recording: %f mins" % (dur/60.)  

	return

def convert_OMNI_from_hdf_singlechannel(data, crc, ramp):
	'''
	This method converts mat files with CRC, data from a single channel, timestamps, and ramps signal. 

	Input:
		- mat_file: string containing the file path for a mat file saved with OMNI device data. mat file should containing a dictionary with keys 'data' (data from single 
			channel), 'timestamps' (timestamps of samples, same length as data signal), 'crc' (parity check for each packet/sample), and 'ramp' (ramp signal, same length as data signal)
		- channel_num: number of the channel that the data comes from, used for file naming at the end
	
	Output:
		- data: pandas DataFrame, M rows x 2 columns, M = number of data points, the first column contains the data samples while the second column 
				contains the timestamps 

	'''
	omni = dict()
	print "Loading data."
	#sp.io.loadmat(mat_file,omni)
	#time_stamps = hdf.root.dataGroup.dataTable[:]['time']
	print "Loaded data."
	#channel_data = [omni['data'], omni['timestamps']]
	channel_data = data
	time_samps = len(data)
	num_col = 1
	#crc_flag = np.array(omni['crc'])
	crc_flag = crc
	print "Finding which "
	ind_crc_pass = np.ravel(np.nonzero(crc_flag==0))
	print "Found which inds pass CRC"
	#channel_data = np.zeros([len(ind_crc_pass),num_col-2])  # 2 fewer columns since one is crv flag and one is ramp

	#counter_ramp = omni['ramp'][ind_crc_pass]
	counter_ramp = ramp

	corrected_counter = [counter_ramp[0]]
	num_cycle = 0
	print "Finding missed samples in ramp."
	# Counter is 16 bit and resets at 2**16. This loop unwraps this cycling so that values are monotonically increasing.
	for i in range(1,len(counter_ramp)):
	#for i in range(1,17000):
		print float(i)/len(counter_ramp)
		diff = counter_ramp[i] - counter_ramp[i-1]
		diff = int(diff)
		
		if (diff == -2**16 +1):
			num_cycle += 1
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
		else:
			corrected_counter.append(counter_ramp[i] + num_cycle*(2**16))
	
	corrected_counter = np.array(corrected_counter)
	diff_corrected_counter = corrected_counter[1:] - corrected_counter[:-1]
	miss_samp_index = np.ravel(np.nonzero(np.greater(diff_corrected_counter,1)))

	corrected_channel_data = channel_data[0:miss_samp_index[0]+1]
	diff = corrected_counter[miss_samp_index[0]+1] - corrected_counter[miss_samp_index[0]]
	diff = int(diff)
	inter_mat = np.zeros(diff)
	
	y = np.interp(range(1,diff),[0, diff], [channel_data[miss_samp_index[0]], channel_data[miss_samp_index[0]+1]])
	y = np.append(y,channel_data[miss_samp_index[0]+1])
	inter_mat = y
	corrected_channel_data = np.append(corrected_channel_data, inter_mat)
	
	print "There are %i instances of missed samples." % len(miss_samp_index)
	print "Beginning looping through regeneration of data"
	for i in range(1,len(miss_samp_index)):
		print float(i)/(len(miss_samp_index)-1)
		# pad with good data first
		corrected_channel_data = np.append(corrected_channel_data, channel_data[miss_samp_index[i-1] + 2:miss_samp_index[i]+1])
		# check number of samples that were skipped and need to be regenerated
		diff = corrected_counter[miss_samp_index[i]+1] - corrected_counter[miss_samp_index[i]]
		diff = int(diff)
		inter_mat = np.zeros(diff)
		# interpolate values to regenerate missing data
		for j in range(0,num_col):
			y = np.interp(range(1,diff),[0, diff], [channel_data[miss_samp_index[i]], channel_data[miss_samp_index[i]+1]])
			y = np.append(y,channel_data[miss_samp_index[i]+1])
			inter_mat = y
		corrected_channel_data = np.append(corrected_channel_data,inter_mat)

	if (miss_samp_index[-1]+1 != len(ind_crc_pass)-1):
		print "adding last zeros"
		corrected_channel_data = np.append(corrected_channel_data, channel_data[miss_samp_index[-1] + 2])
		
	return corrected_counter, corrected_channel_data

def computePowersWithChirplets(channel_data,Avg_Fs,channel,event_indices,t_before, t_after, center_freq):
	'''
	This method extracts spectral amplitudes around a defined center frequency by convolving raw LFP data with Gabor 
	time-frequency basis functions (Gaussian envelope).

	Inputs:
		- channel_data: raw LFP data formatted as a multi-dimensional array of size N_samps x N_channels
		- Avg_Fs: sampling frequency of LFP data
		- channel: channel to perform analysis on, all values should be in range [1,N_channels]
		- event_indices: sample indices that correspond to trial events data is aligned to, one-dimensional of length N_trials 
		- t_before: time before event index to include in analysis, measured in seconds
		- t_after: time after event index to include in analysis, measured in seconds
		- center_freq: center frequency parameter that is used in definition of Gabor atom
	Outputs:
		- trial_powers: power amplitudes formatterd a multi-dimensional array of size N_trials x N_timepoints, where N_timepoints is defined by the window
		                size dictated by the t_before and t_after parameter
		- times: vector of time points for easy plotting after analysis is complete
	'''
	# Define Gabor atom parameters
	v_0 = center_freq
	s_0 = -5.075
	t_0 = 0

	# Define other parameters
	win_before = int(t_before*Avg_Fs)
	win_after = int(t_after*Avg_Fs)
	channel = np.array(channel) - 1 	# adjust so that counting starts at 0
	print "Defining variables"
	times = np.arange(-t_before,t_after,float(t_after + t_before)/(win_after + win_before))
	trial_powers = np.zeros([len(event_indices),2*944])
	windows = np.zeros([len(event_indices),len(times)])

	for i,ind in enumerate(event_indices):
		print i,'/',len(event_indices)
		window = range(int(ind) - win_before,int(ind) + win_after)
		windows[i,:] = window
		#t_0 = -t_before + 1
		gabor_atom = (2**0.25)*np.exp(-0.25*s_0 - np.pi*((times - t_0)**2)*np.exp(-s_0) + 1j*np.pi*(times - t_0)*(2*v_0))
		complex_power = np.convolve(channel_data[window,channel],gabor_atom,mode='full')
		trial_powers[i,:] = np.absolute(complex_power[3*944:5*944]) 	# get power magnitudes
		#trial_powers[i,:] = np.absolute(complex_power) 	# get power magnitudes

	#mlab.specgram(channel_data[window,channel], NFFT=256, Fs=Avg_Fs)
	print "Done looping"
	return trial_powers, times, complex_power, channel_data[window,channel], windows

def powersWithFFT(channel_data,Avg_Fs,channel,event_indices,t_before, t_after):

	win_before = int(t_before*Avg_Fs)
	win_after = int(t_after*Avg_Fs)
	channel = np.array(channel) - 1 	# adjust so that counting starts at 0

	times = np.arange(-t_before,t_after,float(t_after + t_before)/(win_after + win_before))
	window_times = np.arange(-win_before,win_after)
	trial_powers = np.zeros([len(event_indices),len(times)])


	T = 1./Avg_Fs
	N = 256
	x = np.linspace(0.0, N*T, N)
	xf = np.linspace(0.0, 1.0/(2.0*T), N/2)
	for j,ind in enumerate(event_indices[0:10]):
		for i in range(0,len(times)):
			print i,'/',len(times)
			t = np.arange(ind + times[i],ind+256 + times[i],1./Avg_Fs)
			data = channel_data[ind + window_times[i]:ind+256 + window_times[i],channel]/np.sum(channel_data[ind + window_times[i]:ind+256 + window_times[i],channel])
			sp = np.fft.fft(data)
			trial_powers[j,i] = np.absolute(sp[5])**2
	print len(xf)
	print len(sp)
	
	return trial_powers, xf

def powersWithSpecgram(channel_data,Avg_Fs,channel,event_indices,t_before, t_after):

	win_before = int(t_before*Avg_Fs)
	win_after = int(t_after*Avg_Fs)
	channel = np.array(channel) - 1 	# adjust so that counting starts at 0

	times = np.arange(-t_before,t_after,float(t_after + t_before)/(win_after + win_before))
	trial_powers = np.zeros([len(event_indices),28])

	for j,ind in enumerate(event_indices):
		data = channel_data[ind - win_before:ind + win_after,channel]
		data = np.ravel(data)
		Sxx, f, t, fig = specgram(data,Fs=Avg_Fs)
		#Sxx = Sxx/np.sum(Sxx)
		Sxx = 10*np.log10(Sxx)
		trial_powers[j,:] = np.sum(Sxx[3:5,:],axis=0)/2.
		#trial_powers[j,:] = Sxx[5,:]
	return trial_powers, t, f
	#return trial_powers, data

'''
filename_prefix = 'C:/Users/Samantha Summerson/Dropbox/Carmena Lab/OMNI_Device/Data/streams7_20/'
filename = filename_prefix + '20160720-163020.csv'
#filename = filename_prefix + '20160720-171300.csv'
#filename = filename_prefix + '20160720-174338.csv'
data = pd.read_csv(filename,sep=',',header=None,skiprows=[0,1])
print "Data read."
#test_corrected_counter, test_corrected_channel_data, counter = test_convert_OMNI(data)
corrected_counter, corrected_channel_data, channel_data, miss_samp_index = convert_OMNI(data)
'''

def get_OMNI_inds_hold_center(hdf_filename, syncHDF_file, tdt_timepoint, omni_timepoint, omni_freq):
	'''
	Method to get the indices in OMNI recordings of beginning of center hold. 

	Input: 
	- hdf_filename: location of hdf file for the behavior
	- sync_filename: location of sync file
	- tdt_timepoint: a timepoint in tdt recording that can be matched with a timepoint in the omni
					recording, e.g. time of first stimulation pulse
	- omni_timepoint: a timepoint in the OMNI recording that can be matched with a timepoint in the 
					tdt recording
	- omni_freq: sampling frequency of OMNI data, e.g. 1000 = 1 kHz sample rate

	Output:
	- state_row_ind: task timestamps for behavioral events, used to sanity check output
	- omni_inds: sample numbers for OMNI recorded data corresponding to center hold periods
	'''
	
	# Find hdf row numbers corresponding to hold_center times in successful trials only
	table = tables.openFile(hdf_filename)
	state = table.root.task_msgs[:]['msg']
	state_time = table.root.task_msgs[:]['time']
	ind_reward = np.ravel(np.nonzero(state == 'reward'))
	ind_hold_center = ind_reward - 7
	ind_all_hold_center = np.ravel(np.nonzero(state == 'hold_center'))
	ind_peripheral_on = ind_reward - 6
	ind_go_cue = ind_reward - 3
	ind_hold_peripheral = ind_reward - 2
	ind_end_hold_peripheral = ind_reward - 1
	ind_reward_end = ind_reward + 1

	
	
	# Load syncing data
	hdf_times = dict()
	sp.io.loadmat(syncHDF_file, hdf_times)
	hdf_rows = np.ravel(hdf_times['row_number'])
	hdf_rows = [val for val in hdf_rows]
	dio_tdt_sample = np.ravel(hdf_times['tdt_samplenumber'])
	dio_freq = np.ravel(hdf_times['tdt_dio_samplerate'])

	lfp_dio_sample_num = dio_tdt_sample  # assumes DIOx and LFPx are saved using the same sampling rate

	# Get indices for beginning of hold
	state_row_ind = state_time[ind_hold_center]		# gives the hdf row number sampled at 60 Hz
	state_row_ind_beginhold = state_row_ind
	lfp_state_row_ind = np.zeros(state_row_ind.size)

	for i in range(len(state_row_ind)):
		hdf_index = np.argmin(np.abs(hdf_rows - state_row_ind[i]))
		if np.abs(hdf_rows[hdf_index] - state_row_ind[i])==0:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		elif hdf_rows[hdf_index] > state_row_ind[i]:
			hdf_row_diff = hdf_rows[hdf_index] - hdf_rows[hdf_index -1]  # distance of the interval of the two closest hdf_row_numbers
			m = (lfp_dio_sample_num[hdf_index]-lfp_dio_sample_num[hdf_index - 1])/hdf_row_diff
			b = lfp_dio_sample_num[hdf_index-1] - m*hdf_rows[hdf_index-1]
			lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
		elif (hdf_rows[hdf_index] < state_row_ind[i])&(hdf_index + 1 < len(hdf_rows)):
			hdf_row_diff = hdf_rows[hdf_index + 1] - hdf_rows[hdf_index]
			if (hdf_row_diff > 0):
				m = (lfp_dio_sample_num[hdf_index + 1] - lfp_dio_sample_num[hdf_index])/hdf_row_diff
				b = lfp_dio_sample_num[hdf_index] - m*hdf_rows[hdf_index]
				lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
			else:
				lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		else:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]

	lfp_state_row_ind_beginhold = lfp_state_row_ind

	# Get indices for all trials beginning of hold
	state_row_ind = state_time[ind_all_hold_center]		# gives the hdf row number sampled at 60 Hz
	state_row_ind_beginhold = state_row_ind
	lfp_state_row_ind = np.zeros(state_row_ind.size)

	for i in range(len(state_row_ind)):
		hdf_index = np.argmin(np.abs(hdf_rows - state_row_ind[i]))
		if np.abs(hdf_rows[hdf_index] - state_row_ind[i])==0:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		elif hdf_rows[hdf_index] > state_row_ind[i]:
			hdf_row_diff = hdf_rows[hdf_index] - hdf_rows[hdf_index -1]  # distance of the interval of the two closest hdf_row_numbers
			m = (lfp_dio_sample_num[hdf_index]-lfp_dio_sample_num[hdf_index - 1])/hdf_row_diff
			b = lfp_dio_sample_num[hdf_index-1] - m*hdf_rows[hdf_index-1]
			lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
		elif (hdf_rows[hdf_index] < state_row_ind[i])&(hdf_index + 1 < len(hdf_rows)):
			hdf_row_diff = hdf_rows[hdf_index + 1] - hdf_rows[hdf_index]
			if (hdf_row_diff > 0):
				m = (lfp_dio_sample_num[hdf_index + 1] - lfp_dio_sample_num[hdf_index])/hdf_row_diff
				b = lfp_dio_sample_num[hdf_index] - m*hdf_rows[hdf_index]
				lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
			else:
				lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		else:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]

	lfp_state_row_ind_beginhold_all = lfp_state_row_ind

	# Repeat for peripheral target appearing
	state_row_ind = state_time[ind_peripheral_on]
	state_row_ind_peripheralon = state_row_ind
	lfp_state_row_ind = np.zeros(state_row_ind.size)

	for i in range(len(state_row_ind)):
		hdf_index = np.argmin(np.abs(hdf_rows - state_row_ind[i]))
		if np.abs(hdf_rows[hdf_index] - state_row_ind[i])==0:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		elif hdf_rows[hdf_index] > state_row_ind[i]:
			hdf_row_diff = hdf_rows[hdf_index] - hdf_rows[hdf_index -1]  # distance of the interval of the two closest hdf_row_numbers
			m = (lfp_dio_sample_num[hdf_index]-lfp_dio_sample_num[hdf_index - 1])/hdf_row_diff
			b = lfp_dio_sample_num[hdf_index-1] - m*hdf_rows[hdf_index-1]
			lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
		elif (hdf_rows[hdf_index] < state_row_ind[i])&(hdf_index + 1 < len(hdf_rows)):
			hdf_row_diff = hdf_rows[hdf_index + 1] - hdf_rows[hdf_index]
			if (hdf_row_diff > 0):
				m = (lfp_dio_sample_num[hdf_index + 1] - lfp_dio_sample_num[hdf_index])/hdf_row_diff
				b = lfp_dio_sample_num[hdf_index] - m*hdf_rows[hdf_index]
				lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
			else:
				lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		else:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]

	lfp_state_row_ind_peripheralon = lfp_state_row_ind

	# Repeat for end of hold, i.e. GO CUE
	state_row_ind = state_time[ind_go_cue]
	state_row_ind_endhold = state_row_ind
	lfp_state_row_ind = np.zeros(state_row_ind.size)

	for i in range(len(state_row_ind)):
		hdf_index = np.argmin(np.abs(hdf_rows - state_row_ind[i]))
		if np.abs(hdf_rows[hdf_index] - state_row_ind[i])==0:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		elif hdf_rows[hdf_index] > state_row_ind[i]:
			hdf_row_diff = hdf_rows[hdf_index] - hdf_rows[hdf_index -1]  # distance of the interval of the two closest hdf_row_numbers
			m = (lfp_dio_sample_num[hdf_index]-lfp_dio_sample_num[hdf_index - 1])/hdf_row_diff
			b = lfp_dio_sample_num[hdf_index-1] - m*hdf_rows[hdf_index-1]
			lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
		elif (hdf_rows[hdf_index] < state_row_ind[i])&(hdf_index + 1 < len(hdf_rows)):
			hdf_row_diff = hdf_rows[hdf_index + 1] - hdf_rows[hdf_index]
			if (hdf_row_diff > 0):
				m = (lfp_dio_sample_num[hdf_index + 1] - lfp_dio_sample_num[hdf_index])/hdf_row_diff
				b = lfp_dio_sample_num[hdf_index] - m*hdf_rows[hdf_index]
				lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
			else:
				lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		else:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]

	lfp_state_row_ind_endhold = lfp_state_row_ind

	# Repeat for beginning of peripheral hold
	state_row_ind = state_time[ind_hold_peripheral]
	lfp_state_row_ind = np.zeros(state_row_ind.size)

	for i in range(len(state_row_ind)):
		hdf_index = np.argmin(np.abs(hdf_rows - state_row_ind[i]))
		if np.abs(hdf_rows[hdf_index] - state_row_ind[i])==0:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		elif hdf_rows[hdf_index] > state_row_ind[i]:
			hdf_row_diff = hdf_rows[hdf_index] - hdf_rows[hdf_index -1]  # distance of the interval of the two closest hdf_row_numbers
			m = (lfp_dio_sample_num[hdf_index]-lfp_dio_sample_num[hdf_index - 1])/hdf_row_diff
			b = lfp_dio_sample_num[hdf_index-1] - m*hdf_rows[hdf_index-1]
			lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
		elif (hdf_rows[hdf_index] < state_row_ind[i])&(hdf_index + 1 < len(hdf_rows)):
			hdf_row_diff = hdf_rows[hdf_index + 1] - hdf_rows[hdf_index]
			if (hdf_row_diff > 0):
				m = (lfp_dio_sample_num[hdf_index + 1] - lfp_dio_sample_num[hdf_index])/hdf_row_diff
				b = lfp_dio_sample_num[hdf_index] - m*hdf_rows[hdf_index]
				lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
			else:
				lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		else:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]

	lfp_state_row_ind_beginhold_peripheral = lfp_state_row_ind

	# Repeat for end of peripheral hold
	state_row_ind = state_time[ind_end_hold_peripheral]
	state_row_ind_endhold = state_row_ind
	lfp_state_row_ind = np.zeros(state_row_ind.size)

	for i in range(len(state_row_ind)):
		hdf_index = np.argmin(np.abs(hdf_rows - state_row_ind[i]))
		if np.abs(hdf_rows[hdf_index] - state_row_ind[i])==0:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		elif hdf_rows[hdf_index] > state_row_ind[i]:
			hdf_row_diff = hdf_rows[hdf_index] - hdf_rows[hdf_index -1]  # distance of the interval of the two closest hdf_row_numbers
			m = (lfp_dio_sample_num[hdf_index]-lfp_dio_sample_num[hdf_index - 1])/hdf_row_diff
			b = lfp_dio_sample_num[hdf_index-1] - m*hdf_rows[hdf_index-1]
			lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
		elif (hdf_rows[hdf_index] < state_row_ind[i])&(hdf_index + 1 < len(hdf_rows)):
			hdf_row_diff = hdf_rows[hdf_index + 1] - hdf_rows[hdf_index]
			if (hdf_row_diff > 0):
				m = (lfp_dio_sample_num[hdf_index + 1] - lfp_dio_sample_num[hdf_index])/hdf_row_diff
				b = lfp_dio_sample_num[hdf_index] - m*hdf_rows[hdf_index]
				lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
			else:
				lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		else:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]

	lfp_state_row_ind_endhold_peripheral = lfp_state_row_ind

	# Repeat for beginning of reward
	state_row_ind = state_time[ind_reward]
	state_row_ind_endhold = state_row_ind
	lfp_state_row_ind = np.zeros(state_row_ind.size)

	for i in range(len(state_row_ind)):
		hdf_index = np.argmin(np.abs(hdf_rows - state_row_ind[i]))
		if np.abs(hdf_rows[hdf_index] - state_row_ind[i])==0:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		elif hdf_rows[hdf_index] > state_row_ind[i]:
			hdf_row_diff = hdf_rows[hdf_index] - hdf_rows[hdf_index -1]  # distance of the interval of the two closest hdf_row_numbers
			m = (lfp_dio_sample_num[hdf_index]-lfp_dio_sample_num[hdf_index - 1])/hdf_row_diff
			b = lfp_dio_sample_num[hdf_index-1] - m*hdf_rows[hdf_index-1]
			lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
		elif (hdf_rows[hdf_index] < state_row_ind[i])&(hdf_index + 1 < len(hdf_rows)):
			hdf_row_diff = hdf_rows[hdf_index + 1] - hdf_rows[hdf_index]
			if (hdf_row_diff > 0):
				m = (lfp_dio_sample_num[hdf_index + 1] - lfp_dio_sample_num[hdf_index])/hdf_row_diff
				b = lfp_dio_sample_num[hdf_index] - m*hdf_rows[hdf_index]
				lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
			else:
				lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		else:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]

	lfp_state_row_ind_reward = lfp_state_row_ind

	# Repeat for end of reward
	state_row_ind = state_time[ind_reward_end]
	state_row_ind_endhold = state_row_ind
	lfp_state_row_ind = np.zeros(state_row_ind.size)

	for i in range(len(state_row_ind)):
		hdf_index = np.argmin(np.abs(hdf_rows - state_row_ind[i]))
		if np.abs(hdf_rows[hdf_index] - state_row_ind[i])==0:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		elif hdf_rows[hdf_index] > state_row_ind[i]:
			hdf_row_diff = hdf_rows[hdf_index] - hdf_rows[hdf_index -1]  # distance of the interval of the two closest hdf_row_numbers
			m = (lfp_dio_sample_num[hdf_index]-lfp_dio_sample_num[hdf_index - 1])/hdf_row_diff
			b = lfp_dio_sample_num[hdf_index-1] - m*hdf_rows[hdf_index-1]
			lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
		elif (hdf_rows[hdf_index] < state_row_ind[i])&(hdf_index + 1 < len(hdf_rows)):
			hdf_row_diff = hdf_rows[hdf_index + 1] - hdf_rows[hdf_index]
			if (hdf_row_diff > 0):
				m = (lfp_dio_sample_num[hdf_index + 1] - lfp_dio_sample_num[hdf_index])/hdf_row_diff
				b = lfp_dio_sample_num[hdf_index] - m*hdf_rows[hdf_index]
				lfp_state_row_ind[i] = int(m*state_row_ind[i] + b)
			else:
				lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]
		else:
			lfp_state_row_ind[i] = lfp_dio_sample_num[hdf_index]

	lfp_state_row_ind_reward_end = lfp_state_row_ind

	# lfp_state_row_ind now has the TDT sample numbers corresponding to the behavior indices of interest. these
	# are sampled at the rate dio_freq.

	# Translate lfp_state_row_ind to OMNI sample inds using the two corresponding timepoints. 
	m = float(omni_freq)/dio_freq
	b = omni_timepoint - m*tdt_timepoint

	omni_inds_beginhold = np.rint(m*lfp_state_row_ind_beginhold + b) 	# make sure that indices are integer values
	omni_inds_beginhold_all = np.rint(m*lfp_state_row_ind_beginhold_all + b)
	omni_inds_peripheralon = np.rint(m*lfp_state_row_ind_peripheralon + b)
	omni_inds_endhold = np.rint(m*lfp_state_row_ind_endhold + b)
	omni_inds_peripheralhold = np.rint(m*lfp_state_row_ind_beginhold_peripheral + b)
	omni_inds_endhold_peripheral = np.rint(m*lfp_state_row_ind_endhold_peripheral + b)
	omni_inds_reward = np.rint(m*lfp_state_row_ind_reward + b)
	omni_inds_reward_end = np.rint(m*lfp_state_row_ind_reward_end + b)

	return omni_inds_beginhold, omni_inds_beginhold_all, omni_inds_peripheralon,omni_inds_endhold, omni_inds_peripheralhold, omni_inds_endhold_peripheral, omni_inds_reward, omni_inds_reward_end 

def ClosedLoopReactionTimeAnalysis(hdf_filename, task_events_mat_filename, stim_filename, thres, lower_rt, upper_rt, method):
	'''
	This method finds which trials have stimulation during the peripheral hold period of the delayed reach task
	and at what time stimulation occurs relative to the Go Cue.

	Inputs:
	- hdf_filename: string containing hdf filename and location
	- task_events_mat_filename: string containing .mat filename that contains task events indices in terms of OMNI timestamps
	- stim_filename: string containing .mat filename that contains stimulation start and stop times in terms of OMNI timestamps
	- thres: velocity threshold value used for computing reaction time
	- method: 1 or 2
	'''

	# 1. Compute reaction times
	rt, total_vel, skipped_trials = compute_rt_per_trial_CenterOut(hdf_filename, method, thres, False)
	all_rt = np.zeros(len(rt) + len(skipped_trials))
	good_rt = [ind for ind in range(len(all_rt)) if ind not in skipped_trials]
	all_rt[good_rt] = rt

	# Filter trials by reaction time: only consider when rt in the range (0.1, 1) sec
	too_short = np.ravel(np.nonzero(np.less(rt,lower_rt)))
	too_long = np.ravel(np.nonzero(np.greater(rt,upper_rt)))
	bad_trial_inds = np.ravel(np.append(too_short, too_long))
	good_trial_inds = [ind for ind in range(len(all_rt)) if ind not in bad_trial_inds]  	# take only trials where reaction time is in allotted window
	rt_good_trials = all_rt[good_trial_inds]

	# 2. Get OMNI task event times
	task_events = dict()
	sp.io.loadmat(task_events_mat_filename, task_events)
	omni_time_periph_on = np.ravel(task_events['peripheral_on'])					# array of time when peripheral target is shown during center hold
	omni_time_center_hold_begin = np.ravel(task_events['begin_center_hold'])		# array of time when center hold begins
	omni_time_go_cue = np.ravel(task_events['go_cue'])							# array of time when center hold ends
	omni_time_end_reward = np.ravel(task_events['end_reward'])

	time_periph_on = omni_time_periph_on[good_trial_inds]
	time_center_hold_begin = omni_time_center_hold_begin[good_trial_inds]
	time_go_cue = omni_time_go_cue[good_trial_inds]
	time_end_reward = omni_time_end_reward[good_trial_inds]

	# 3. Get stim times
	stims = dict()
	sp.io.loadmat(stim_filename, stims)
	trains = stims['trains']
	stim_start = trains[:,0]
	stim_end = trains[0,1]

	# 4. For each trial, find stim time closest to peripheral on
	time_after_periph = np.zeros(len(good_trial_inds))
	time_before_go_cue = np.zeros(len(good_trial_inds))
	is_during_periph_on = np.zeros(len(good_trial_inds))
	is_during_center_on_alone = np.zeros(len(good_trial_inds))

	ind_during_periph_on = []
	ind_not_during_periph_on = []
	ind_during_center_on = []
	ind_no_stim_trial = []

	for val in range(len(good_trial_inds)):
		# find closest stim
		stim_index = np.argmin(np.abs(stim_start - time_periph_on[val]))
		# find time relative to periph_on: positive is after, negative is before
		time_after_periph[val] = stim_start[stim_index] - time_periph_on[val]
		time_before_go_cue[val] = stim_start[stim_index] - time_go_cue[val]
		is_during_periph_on[val] = (time_after_periph[val] > -0.067)&(time_after_periph[val] < (time_go_cue[val] - time_periph_on[val]))
		is_during_center_on_alone[val] = (time_after_periph[val] < -0.067)&(time_after_periph[val] > (time_center_hold_begin[val] - time_periph_on[val]))

		# build lists of indices for if stim occured when peripheral was shown or not
		if is_during_periph_on[val]:
			ind_during_periph_on.append(val)
		elif is_during_center_on_alone[val]:
			ind_during_center_on.append(val)
		else:
			ind_not_during_periph_on.append(val)

		# did any stim actually happen between the beginning of the hold and end of reward?
		trial_midpoint = (time_center_hold_begin[val] + time_end_reward[val])/2
		stim_index = np.argmin(np.abs(stim_start - trial_midpoint))
		before_center = time_center_hold_begin[val] - stim_start[stim_index]
		after_reward = stim_start[stim_index] - time_end_reward[val]
		if (before_center > 0)&(after_reward > 0):
			ind_no_stim_trial.append(val)

	time_after_periph = time_after_periph/1000. 	# convert from samples to seconds
	time_before_go_cue = time_before_go_cue/1000.

	# 5. Pull out reaction times for different conditions
	rt_stim_during_periph = rt_good_trials[ind_during_periph_on]											# triasl when stim is during peripheral hold
	rt_stim_not_during_periph = rt_good_trials[np.append(ind_not_during_periph_on, ind_during_center_on)] 	# all trials except when stim is on during peripheral hold
	rt_stim_during_center = rt_good_trials[ind_during_center_on]											# trials when stim is during center on, before peripheral is shown
	rt_stim_not_during_hold = rt_good_trials[ind_not_during_periph_on]										# all trials except when stim is during any time in the hold


	return rt_stim_during_periph, rt_stim_not_during_periph, rt_stim_during_center, rt_stim_not_during_hold, time_after_periph, time_before_go_cue,rt_good_trials, ind_no_stim_trial, good_trial_inds

def ClosedLoopReactionTimeAnalysis_MultipleSessions(hdf_filename, task_events_mat_filename, stim_filename, thres,lower_rt, upper_rt, method):
	'''
	This method finds which trials have stimulation during the peripheral hold period of the delayed reach task
	and at what time stimulation occurs relative to the Go Cue. It takes in data across multiple behavior sessions

	Inputs:
	- hdf_filename: list of strings containing hdf filename and location
	- task_events_mat_filename: list of strings containing .mat filename that contains task events indices in terms of OMNI timestamps
	- stim_filename: list of strings containing .mat filename that contains stimulation start and stop times in terms of OMNI timestamps
	- thres: float value used to set threshold for finding reaction times
	
	To do: (1) add stats
	'''
	num_sessions = len(hdf_filename)
	rt_stim_during_periph = np.array([])
	rt_stim_not_during_periph = np.array([])
	rt_stim_during_center = np.array([])
	rt_stim_not_during_hold = np.array([])
	time_after_periph = np.array([])
	time_before_go_cue = np.array([])
	rt_good_trials = np.array([])
	ind_no_stim_trial = np.array([])
	num_trials_per_session = np.array([])
	inds_good_trials = np.array([])

	for k in range(num_sessions):
		output = ClosedLoopReactionTimeAnalysis(hdf_filename[k], task_events_mat_filename[k], stim_filename[k], thres,lower_rt, upper_rt, method)
		rt_stim_during_periph = np.append(rt_stim_during_periph, output[0])
		rt_stim_not_during_periph = np.append(rt_stim_not_during_periph, output[1])
		rt_stim_during_center = np.append(rt_stim_during_center, output[2])
		rt_stim_not_during_hold = np.append(rt_stim_not_during_hold, output[3])
		time_after_periph = np.append(time_after_periph, output[4])
		time_before_go_cue = np.append(time_before_go_cue, output[5])
		rt_good_trials = np.append(rt_good_trials, output[6])
		ind_no_stim_trial = np.append(ind_no_stim_trial, output[7])
		inds_good_trials = np.append(inds_good_trials, output[8])
		num_trials_per_session = np.append(num_trials_per_session, len(output[6]))

	print "Number of good trials:", len(rt_good_trials)
		
	print ind_no_stim_trial
	avg_rt_stim_during_periph = np.nanmean(rt_stim_during_periph)
	avg_rt_not_during_periph = np.nanmean(rt_stim_not_during_periph)
	avg_rt_stim_during_center = np.nanmean(rt_stim_during_center)
	avg_rt_stim_not_during_hold = np.nanmean(rt_stim_not_during_hold)

	all_stim_in_hold = np.append(rt_stim_during_periph, rt_stim_during_center)
	avg_rt_stim_during_hold = np.nanmean(all_stim_in_hold)
	sem_rt_stim_during_hold = np.nanstd(all_stim_in_hold)/np.sqrt(len(all_stim_in_hold))

	if not ind_no_stim_trial:
		avg_rt_no_stim_trial = np.nan
		sem_rt_no_stim_trial = np.nan
	else:
		avg_rt_no_stim_trial = np.nanmean(rt_good_trials[ind_no_stim_trial])
		sem_rt_no_stim_trial = np.nanstd(rt_good_trials[ind_no_stim_trial])/np.sqrt(len(ind_no_stim_trial))
	sem_rt_stim_during_periph = np.nanstd(rt_stim_during_periph)/np.sqrt(len(rt_stim_during_periph))
	sem_rt_not_during_periph = np.nanstd(rt_stim_not_during_periph)/np.sqrt(len(rt_stim_not_during_periph))
	sem_rt_stim_during_center = np.nanstd(rt_stim_during_center)/np.sqrt(len(rt_stim_during_center))
	sem_rt_stim_not_during_hold = np.nanstd(rt_stim_not_during_hold)/np.sqrt(len(rt_stim_not_during_hold))
	
	
	# 6. Do statistics
	# t-test
	t_periph_nothold, p_periph_nothold = stats.ttest_ind(rt_stim_during_center, rt_stim_not_during_hold)
	t_periph_notperiph, p_periph_notperiph = stats.ttest_ind(rt_stim_during_periph, rt_stim_not_during_periph)

	# mann-whitney
	stat_periph_nothold, mw_p_periph_nothold = stats.mannwhitneyu(rt_stim_during_center, rt_stim_not_during_hold)
	stat_periph_notperiph, mw_p_periph_notperiph = stats.mannwhitneyu(rt_stim_during_periph, rt_stim_not_during_periph)

	# kruskal-wallis h-test
	h_periph_nothold, kw_p_periph_nothold = stats.kruskal(rt_stim_during_center, rt_stim_not_during_hold)
	h_periph_notperiph, kw_p_periph_notperiph = stats.kruskal(rt_stim_during_periph, rt_stim_not_during_periph)


	print "T-test Results: \n Periph vs Not During Hold - (t, p) = (%f,%f)" % (t_periph_nothold, p_periph_nothold)
	print " Periph vs Not During Periph - (t, p) = (%f,%f)" % (t_periph_notperiph, p_periph_notperiph)
	print "\nMann-Whitney Results:\n Periph vs Not Durhing Hold - (s, p) = (%f,%f)" % (stat_periph_nothold, mw_p_periph_nothold)
	print " Periph vs Not During Periph - (s, p) = (%f,%f)" % (stat_periph_notperiph, mw_p_periph_notperiph)
	print "\nKruskal-Wallis Results: \n Periph vs Not During Hold - (h, p) = (%f,%f)" % (h_periph_nothold, kw_p_periph_nothold)
	print " Periph vs Not During Periph - (h, p) = (%f,%f)" % (h_periph_notperiph, kw_p_periph_notperiph)


	thresname = float(thres*10)
  	lower_rt_name = float(lower_rt*100)
  	upper_rt_name = float(upper_rt*10)
  	plt_name = 'DelayedReach_RT_Method' + str(method) + '_Thres' + str(thresname) + '_lowerRT' + str(lower_rt_name) + 'upperRT' + str(upper_rt_name) + '.svg'
  	
  	print plt_name
  	print "RT change:", avg_rt_stim_during_center - avg_rt_stim_not_during_hold

	# 6. Plot results
	avg_rt = [avg_rt_stim_during_periph, avg_rt_not_during_periph, avg_rt_stim_during_center, avg_rt_stim_not_during_hold, avg_rt_no_stim_trial, avg_rt_stim_during_hold]
	sem_rt = [sem_rt_stim_during_periph, sem_rt_not_during_periph, sem_rt_stim_during_center, sem_rt_stim_not_during_hold, sem_rt_no_stim_trial, sem_rt_stim_during_hold]
	plt.figure()
	plt.errorbar(range(6),avg_rt,yerr = sem_rt,fmt = 'o', color = 'k', ecolor = 'k')
	xticklabels = ['Stim during Peripheral', 'Stim not during Peripheral', 'Stim during Center', 'Stim not during hold', 'No Stim', 'Stim during hold']
  	plt.xticks(range(6), xticklabels)
  	plt.xlim((-0.5,5.5))
  	plt.ylabel('Reaction time (s)')
  	plt.title('Average Reaction Time - %s' % (plt_name))
  	plt.text(0,1.1*np.nanmax(avg_rt), "Number of good trials: %i" % (len(rt_good_trials)))
  	plt.text(0,1.0*np.nanmax(avg_rt), "RT change = %f" % (avg_rt_stim_during_center - avg_rt_stim_not_during_hold))
  	plt.savefig(plt_name)
	
	
  	# sort time_after_periph
  	sorted_time_inds = np.argsort(time_after_periph)
  	#time_bins = np.linspace(np.amin(time_after_periph),np.amax(time_after_periph),21)
  	t_min = -0.4
  	t_max = 0.4
  	nbins = 6
  	stepsize = 0.2	# 100 ms
  	#time_bins = np.linspace(t_min,t_max,nbins)		# center hold is 200 ms before peripheral, max peripheral hold is 400 ms
  	time_bins = np.arange(t_min,t_max+stepsize,stepsize)
  	bin_centers = (time_bins[:-1] + time_bins[1:])/2.
  	
	avg_RT_binned = np.zeros(len(bin_centers))
	sem_RT_binned = np.zeros(len(bin_centers))
	RT_binned = dict()

	for j in range(len(bin_centers)):
		rt_inds = np.ravel(np.nonzero(np.greater(time_after_periph, time_bins[j])&np.less(time_after_periph,time_bins[j+1])))	
		RT_binned[j] = rt_good_trials[rt_inds]
		
		avg_RT_binned[j] = np.nanmean(RT_binned[j])
		sem_RT_binned[j] = np.nanstd(RT_binned[j])/np.sqrt(len(RT_binned[j]))

	# Sliding Average
	N = 50
	sorted_rt = rt_good_trials[sorted_time_inds]
	mean_sorted_rt = np.convolve(sorted_rt, np.ones((N,))/N)[N-1:]
	#mean_time_after_periph = np.convolve(time_after_periph[sorted_time_inds], np.ones((N,))/N)[N-1:]

	'''
  	plt.figure(0)
  	plt.subplot(1,2,1)
  	plt.scatter(time_after_periph, rt_good_trials, c = 'c')
  	plt.errorbar(bin_centers, avg_RT_binned, yerr = sem_RT_binned, color = 'k', ecolor = 'k')
  	plt.plot(time_after_periph[sorted_time_inds], mean_sorted_rt, 'm')
  	plt.xlabel('Time relative to Peripheral On (s)')
  	plt.xlim((t_min*1.05,t_max*1.05))
  	plt.ylim((lower_rt,upper_rt))
  	plt.ylabel('Reaction time (s)')
  	'''

  	# sort time_before_go_cue
  	sorted_time_inds_before_gocue = np.argsort(time_before_go_cue)
  	#time_bins = np.linspace(np.amin(time_after_periph),np.amax(time_after_periph),21)
  	t_min = -0.6
  	t_max = 0.4
  	nbins = 6
  	stepsize = 0.2		# 100 ms
  	#time_bins = np.linspace(t_min,t_max,nbins)		# center hold is 200 ms before peripheral, max peripheral hold is 400 ms
  	time_bins = np.arange(t_min,t_max+stepsize,stepsize)
  	bin_centers_before_gocue = (time_bins[:-1] + time_bins[1:])/2.
	avg_RT_binned_before_gocue = np.zeros(len(bin_centers_before_gocue))
	sem_RT_binned_before_gocue = np.zeros(len(bin_centers_before_gocue))
	RT_binned_before_gocue = dict()

	for j in range(len(bin_centers_before_gocue)):
		rt_inds = np.ravel(np.nonzero(np.greater(time_before_go_cue, time_bins[j])&np.less(time_before_go_cue,time_bins[j+1])))	
		RT_binned_before_gocue[j] = rt_good_trials[rt_inds]

		avg_RT_binned_before_gocue[j] = np.nanmean(RT_binned_before_gocue[j])
		sem_RT_binned_before_gocue[j] = np.nanstd(RT_binned_before_gocue[j])/np.sqrt(len(RT_binned_before_gocue[j]))

	# Sliding Average
	N = 50
	sorted_rt_before_gocue = rt_good_trials[sorted_time_inds_before_gocue]
	mean_sorted_rt_before_gocue = np.convolve(sorted_rt_before_gocue, np.ones((N,))/N)[N-1:]
	#mean_time_after_periph = np.convolve(time_after_periph[sorted_time_inds], np.ones((N,))/N)[N-1:]

	'''
  	plt.figure(0)
  	plt.subplot(1,2,2)
  	plt.scatter(time_before_go_cue, rt_good_trials, c = 'c')
  	plt.errorbar(bin_centers_before_gocue, avg_RT_binned_before_gocue, yerr = sem_RT_binned_before_gocue, color = 'k', ecolor = 'k')
  	plt.plot(time_before_go_cue[sorted_time_inds_before_gocue], mean_sorted_rt_before_gocue, 'm')
  	plt.xlabel('Time relative to Go Cue (s)')
  	plt.xlim((t_min*1.05,t_max*1.05))
  	plt.ylim((lower_rt,upper_rt))
  	plt.ylabel('Reaction time (s)')
  	#plt.show()
	'''
  	
  	thresname = float(thres*10)
  	lower_rt_name = float(lower_rt*100)
  	upper_rt_name = float(upper_rt*10)
  	plt_name = 'DelayedReach_RT_Method' + str(method) + '_Thres' + str(thresname) + '_lowerRT' + str(lower_rt_name) + 'upperRT' + str(upper_rt_name) + '.png'
  	
  	
  	# Plot RT distributions: stim during peripheral vs stim not during peripheral
  	# First, fit with log-normal distribution
  	x_stim_during_periph, pdf_stim_during_periph, cdf_stim_during_periph = rt_lognormal_fit(rt_stim_during_periph)
  	x_stim_not_during_periph, pdf_stim_not_during_periphd, cdf_stim_not_during_periph = rt_lognormal_fit(rt_stim_not_during_periph)
	x_stim_not_during_hold, pdf_stim_not_during_hold, cdf_stim_not_during_hold = rt_lognormal_fit(rt_stim_not_during_hold)
	x_stim_not_during_hold_gamma, pdf_stim_not_during_hold_gamma, cdf_stim_not_during_hold_gamma = rt_gamma_fit(rt_stim_not_during_hold)
	x_stim_during_center, pdf_stim_during_center, cdf_stim_during_center = rt_lognormal_fit(rt_stim_during_center)
	x_stim_during_center_gamma, pdf_stim_during_center_gamma, cdf_stim_during_center_gamma = rt_gamma_fit(rt_stim_during_center)

	hist_rt_during_center, bins_during_center = np.histogram(rt_stim_during_center,bins=10)
	weights = np.ones_like(rt_stim_during_center)/len(rt_stim_during_center)

	plt_name = 'DelayedReach_RT_Method' + str(method) + '_Thres' + str(thresname) + '_lowerRT' + str(lower_rt_name) + 'upperRT' + str(upper_rt_name) + '_hold.svg'
  	bins = np.arange(0,1+1./15,1./15)

  	plt.figure()
  	plt.hist(rt_stim_during_center,bins=bins,normed=0,color = 'c',alpha = 0.75, weights = weights)
  	#plt.plot(x_stim_during_center,pdf_stim_during_center,'g', label = 'Hold - lognormal')
  	#plt.plot(x_stim_during_center_gamma, pdf_stim_during_center_gamma,'b', label = 'Hold - gamma')
  	plt.title('PDF of Reaction Times for Stim During Hold')
  	plt.xlabel('Reaction Time (s)')
  	plt.ylabel('Probability')
  	plt.legend()
  	plt.xlim((0,1))
  	#plt.ylim((0,6))
  	plt.savefig(plt_name)

  	plt_name = 'DelayedReach_RT_Method' + str(method) + '_Thres' + str(thresname) + '_lowerRT' + str(lower_rt_name) + 'upperRT' + str(upper_rt_name) + '_not_hold.svg'
  	
  	print 'Not hold = ', len(rt_stim_not_during_hold)
  	print 'Hold = ', len(rt_stim_during_center)
  	plt.figure()
  	plt.hist(rt_stim_not_during_hold,bins=bins,normed=0,color = 'm',alpha = 0.75)
  	#plt.plot(x_stim_not_during_hold,pdf_stim_not_during_hold,'r', label = 'Not hold-lognormal')
  	#plt.plot(x_stim_not_during_hold_gamma, pdf_stim_not_during_hold_gamma,'k', label = 'Not hold - gamma')
  	plt.title('PDF of Reaction Times for Stim Not During Hold')
  	plt.xlabel('Reaction Time (s)')
  	plt.ylabel('Probability')
  	plt.legend()
  	plt.xlim((0,1))
  	#plt.ylim((0,6))
  	plt.savefig(plt_name)

  	'''
  	rt_stim_during_periph_sorted_inds = np.argsort(rt_stim_during_periph)
  	rt_stim_not_during_periph_sorted_inds = np.argsort(rt_stim_not_during_periph)
  	rt_stim_not_during_hold_sorted_inds = np.argsort(rt_stim_not_during_hold)


  	plt.figure(5)
  	plt.plot(rt_stim_during_periph[rt_stim_during_periph_sorted_inds],np.arange(len(rt_stim_during_periph))/float(len(rt_stim_during_periph)), 'c',label = 'During Peripheral')
  	plt.plot(x_stim_during_periph,cdf_stim_during_periph,'k',label='During Peripheral Fit')
  	plt.plot(rt_stim_not_during_periph[rt_stim_not_during_periph_sorted_inds],np.arange(len(rt_stim_not_during_periph))/float(len(rt_stim_not_during_periph)), 'b',label = 'Not During Peripheral')
  	plt.plot(x_stim_not_during_periph,cdf_stim_not_during_periph,'m',label='Not During Peripheral Fit')
  	plt.plot(rt_stim_not_during_hold[rt_stim_not_during_hold_sorted_inds],np.arange(len(rt_stim_not_during_hold))/float(len(rt_stim_not_during_hold)), 'y',label = 'Not During Hold')
  	plt.plot(x_stim_not_during_hold,cdf_stim_not_during_hold,'g',label='Not During Hold Fit')
  	
  	plt.xlabel('Reaction Time (s)')
  	plt.ylabel('Fraction of Trials')
  	plt.legend()
  	plt.ylim((-0.05,1.05))
  	plt.show()
	
  	#### add: find trials completely without stimulation to compare with
	return RT_binned, bin_centers, RT_binned_before_gocue, bin_centers_before_gocue
	'''
	return rt_good_trials, num_trials_per_session, inds_good_trials

def rt_lognormal_fit(rt):
	s, loc, scale = stats.lognorm.fit(rt, floc = 0)
	xmin = rt.min()
  	xmax = rt.max()
  	x = np.linspace(xmin,xmax,100)
  	pdf = stats.lognorm.pdf(x,s,scale = scale)
  	cdf = np.cumsum(pdf)/np.sum(pdf)

	return x, pdf, cdf

def rt_gamma_fit(rt):
	dist = getattr(sp.stats,'gamma')
	params = dist.fit(rt)
	xmin = rt.min()
  	xmax = rt.max()
  	x = np.linspace(xmin,xmax,100)
  	pdf = dist.pdf(x,*params[:-2], loc = params[-2], scale = params[-1])
  	cdf = np.cumsum(pdf)/np.sum(pdf)

	return x, pdf, cdf


def OMNI_sleep_power(data, Wn_start, Wn_stop, win_size):
	'''
	Compute power using OMNI data over frequency window defined by [Wn_start, Wn_stop].

	Inputs:
	- data: array of N time domain samples
	- Wn_start: float indicating the value of the beginning of the bandpass filter (Hz)
	- Wn_stop: float indicating the value of the end of the bandpass filter (Hz)
	- win_size: integer indicating the length of the sliding window using to compute power
	
	Output:
	- time: array indicating time points for powers computed
	- power: array indicating power for frequency band indicated by inputs
	'''
	# 1. Filter data into designated band.
	Fs = 1000.
	nyq = 0.5*Fs
	Wn_start = Wn_start/nyq
	Wn_stop = Wn_stop/nyq
	order = 3

	b, a = signal.butter(order, [Wn_start, Wn_stop], btype= 'bandpass', analog = False)
	filtered_data = signal.filtfilt(b,a,data)

	# 2. Compute power over sliding windows
	t_len = len(data)/Fs 	# len in seconds of data
	win_size = float(win_size)
	count = 0

	power = np.zeros(len(data) - int(win_size))
	for j in range(int(win_size),len(data)):
		power[j-win_size] = np.sum((filtered_data[j- int(win_size):j])**2)/win_size
		count += 1

	# 3. Smooth output
	boxcar_length = 10000.
	boxcar_window = signal.boxcar(boxcar_length)  # 2 bins before, 2 bins after for boxcar smoothing
	smooth_power = np.convolve(power, boxcar_window,mode='same')/boxcar_length

	time = np.arange(0,t_len,t_len/len(power))
	return time, power, smooth_power

def OMNI_to_TDT_stim_indices(OMNI_channel, omni_timepoints, tdt_timepoints, tdt_Fs):
	'''
	This method finds the stim flags in the OMNI data and convert the time indices to the TDT timescale.

	Inputs:
	- OMNI_channel: array of L floats representing OMNI data from one channel
	- omni_timepoints: array of 2 floats, representing two sample indices for stim points in OMNI recording
	- lfp_timepoints: array of 2 floats, representing two sample indices for stim points in the TDT recording
	- tdt_Fs: float representing the sampling frequency of the TDT recordings

	Outputs:
	- tdt_stim_flags: array of M ints representing indices in TDT recordings when stim occurred with the OMNI device
	'''

	# 1. Get stim flags from OMNI_channel data
	stim_flags = np.ravel(np.nonzero(np.greater(OMNI_channel, 2**15)))

	# 2. Translate stim flags from OMNI sampling rate to TDT sampling rate
	m = float(tdt_timepoints[1] - tdt_timepoints[0])/(omni_timepoints[1] - omni_timepoints[0])
	b = tdt_timepoints[1] - m*omni_timepoints[1]
	tdt_stim_flags = np.rint(m*stim_flags + b)

	return tdt_stim_flags


def FindTDTArtifact(TDT_channel, tdt_Fs, thres):
	nyq = 0.5*tdt_Fs
	order = 3
	cutoff = 3000
	normal_cutoff = cutoff / nyq

	b, a = signal.butter(order, normal_cutoff, btype= 'highpass', analog = False)
	filtered_data = signal.filtfilt(b,a,TDT_channel)

	above_thres = np.ravel(np.nonzero(np.greater(filtered_data, thres)))
	tdt_stim_flags = np.array([above_thres[j] for j in range(1,len(above_thres)) if ((above_thres[j]-above_thres[j-1])>5)])

	return tdt_stim_flags

def TDT_stim_psth(TDT_channel, tdt_Fs, len_psth_before, len_psth_after, thres):
	'''
	This method finds the stim flags in the OMNI data and convert the time indices to the TDT timescale.

	Inputs:
	- TDT_channel: array of M floats representing TDT data recorded simultaneously
	- tdt_Fs: float representing the sampling frequency of the TDT recordings
	- len_psth_before: integer number of samples used before the stim flags
	- len_psth_after: integer number of samples used after the stim flags
	'''
	tdt_stim_flags = FindTDTArtifact(TDT_channel, tdt_Fs, thres)
	num_stim_flags = len(tdt_stim_flags)
	tdt_stim_artifact = np.zeros((num_stim_flags, len_psth_before + len_psth_after))
	for j, val in enumerate(tdt_stim_flags):
		tdt_stim_artifact[j,:] = TDT_channel[val-len_psth_before:val+len_psth_after]

	avg_stim_artifact = np.nanmean(tdt_stim_artifact, axis = 0)
	std_stim_artifact = np.nanstd(tdt_stim_artifact, axis = 0)

	plt.plot(avg_stim_artifact)
	plt.show()

	return tdt_stim_flags, tdt_stim_artifact, avg_stim_artifact, std_stim_artifact

def comparePSD(TDT_tank, omni_mat, TDT_channel_list, omni_channel_list, cutoff, len_windows, num_wins):
	'''
	This method computes averaged PSDs for TDT channels and compared them to corresponding PSDs for OMNI channels.

	Inputs:
	- TDT_tanks
	- omni_filename
	- TDT_channel_list
	- omni_channel_list
	- cutoff
	- len_windows: duration in seconds of time windows
	- nun_windows
	'''

	# 1. Load TDT data and extract specified channels
	tdt_lfp = dict()
	tdt_lfp1 = np.array([chan for chan in TDT_channel_list if chan < 97])
	tdt_lfp2 = np.array([(chan-96) for chan in TDT_channel_list if chan > 96])
	for sig in TDT_tank.segments[0].analogsignals:
		if sig.name[:4] == 'LFP1':
			if (sig.channel_index in tdt_lfp1):
				tdt_lfp[sig.channel_index] = np.array(sig)
				Fs = np.float(sig.sampling_rate)
		elif sig.name[:4] == 'LFP2':
			if (sig.channel_index in tdt_lfp2):
				tdt_lfp[sig.channel_index + 96] = np.array(sig)

	# 2. Load OMNI data and extract specified channels.
	#out = omni_hdf.root.dataGroup.dataTable[:]['out']
	out = omni_mat['corrected_data']
	omni_lfp = out[:,omni_channel_list - 1]
	#omni_lfp = out[:,omni_channel_list]
	omni_fs = 1000

	# 3. Compute PSDs
	num_pairs = np.min([len(TDT_channel_list), len(omni_channel_list)])
	tdt_keys = tdt_lfp.keys()
	for i in range(num_pairs):
		tdt_data = tdt_lfp[tdt_keys[i]]
		omni_data = omni_lfp[:,i]
		freq_tdt, Pxx_avg_tdt, Pxx_sem_tdt = averagedPSD(tdt_data, Fs, cutoff, int(len_windows*Fs), num_wins, notch = True)
		freq_omni, Pxx_avg_omni, Pxx_sem_omni = averagedPSD(omni_data, omni_fs, cutoff, int(len_windows*omni_fs), num_wins, notch = False)

		plt.figure()
		spl = plt.subplot(111)
		spl.plot(freq_tdt, Pxx_avg_tdt, 'b', label = 'TDT')
		spl.fill_between(freq_tdt, Pxx_avg_tdt - Pxx_sem_tdt, Pxx_avg_tdt + Pxx_sem_tdt, facecolor = 'blue', alpha = 0.5)
		spl.plot(freq_omni, Pxx_avg_omni, 'r', label = 'OMNI')
		spl.fill_between(freq_omni, Pxx_avg_omni - Pxx_sem_omni, Pxx_avg_omni + Pxx_sem_omni, facecolor = 'red', alpha = 0.5)
		spl.set_yscale('log')
		spl.set_xscale('log')
		plt.xlabel('Frequency (Hz)')
		plt.ylabel('Normalized PSD')
		spl.grid(True)
		plt.legend()
		plt.xlim((-1,cutoff))
		plt.title('TDT Channel %i' %(tdt_keys[i]))
		plt.show()


	return 

def findStimStartAndStop(omni_hdf):
	'''
	This method takes in an OMNI hdf file corresponding to closed-loop stim recordings and find the start and stop times
	of stimulation trains.

	Inputs:
	- omni_hdf: string containing hdf filename for OMNI recording

	Outputs:
	- trains: N x 2 array containing the stim sample start and stop times in each column, where N is the number of stim trains
	'''
	# Load data
	hdf = tables.openFile(omni_hdf)
	out = hdf.root.dataGroup.dataTable[:]['out']

	# Find stim pulses and then trains
	stim_inds = np.ravel(np.nonzero(np.greater(out[:,1], 2**15)))
	stim_start = np.array([stim_inds[i+1] for i in range(len(stim_inds)-1) if (stim_inds[i+1] - stim_inds[i] > 5)])
	stim_start = np.append(stim_inds[0], stim_start)
	stim_end = np.array([stim_inds[i] for i in range(len(stim_inds)-1) if (stim_inds[i+1] - stim_inds[i] > 5)])
	stim_end = np.append(stim_end, stim_inds[-1])

	# Form matrix
	trains = np.vstack([stim_start, stim_end]).T  	# should generate N x 2 array, where N is the number of stim trains
	stims = dict()
	stims['trains'] = trains

	# Save data
	filename = omni_hdf[:-4] + '_stims.mat'
	sp.io.savemat(filename, stims)

	return trains

def AnalyzeOnlinePowerComputations(session, f_low, f_high):
	'''
	Method to extract online computed power recorded using OMNI and relate it to task variables. Outputs
	reaction time and beta power information, as well as power computed in a band specified by [f_low, f_high].

	Input: 
	- session: .mat filename for .mat file generated by Andy's CompileSessionX code.
	- f_low: float representing the lower cutoff frequency of a window of interest (Hz)
	- f_high: float representing the higher cutoff frequency of a window of interest (Hz)
	'''
	
	# Load data and pull out information into arrays
	mat = sp.io.loadmat(session, squeeze_me = True)
	data = mat['session']
	behavior = np.ravel(data['behavior'])[0]			# behavior matrix, each row represents a trial, each column the timing of each event
	betaF = np.ravel(data['betaF'])[0] 					# FFT bins used for beta calculation
	ctrl = np.ravel(data['ctrl'])[0]					# control channel signal
	endFFT = np.ravel(data['endFFT'])[0]				# indices of the last sample of each computed FFT
	threshPower = np.ravel(data['threshPower'])[0]		# power threshold used for closed loop
	threshDeriv = np.ravel(data['threshDeriv'])[0]		# power derivative threshold for closed loop
	NFFT = np.ravel(data['NFFT'])[0]					# FFT size
	omniSpec = np.ravel(data['omniSpec'])[0]			# compiled online computed power spectrums (endFFT contains timing info)
	stim = np.ravel(data['stim'])[0]					# stim channel signal
	stimFlags = np.ravel(data['stimFlags'])[0]			# stim flags (0 = no stim, 1 = stim)
	stimEvents = np.ravel(data['stimEvents'])[0]		# indices of beginning and end of each stim pulse
	stimIdx = np.ravel(data['stimIdx'])[0]				# indices of all stim flags
	t = np.ravel(data['t'])[0]							# time vector
	trains = np.ravel(data['trains'])[0]				# indices of beginning and end of each stim pulse train
	trialSuccess = np.ravel(data['trialSuccess'])[0]	# indices of the successful trials
	centerHoldAll = np.ravel(data['centerHoldAll'])[0]  # center hold for ALL (successful and unsuccessful) trials
	rtInd = np.ravel(data['rtInd'])[0]					# indices of trial times (index into centerHoldAll) for reaction times
	rt = np.ravel(data['rt'])[0]						# reaction times

	# Find time samples corresponding to reaction time computations
	rt_times = centerHoldAll[rtInd]

	# Find average RT
	avg_rt = np.nanmean(rt)

	# Compute beta power from spectral data saved
	beta_power = np.sqrt(np.sum(omniSpec[:, betaF[0]:betaF[1]],1))	# power is in lsb/sqrt(Hz)
	beta_power = 0.0954*beta_power 									# convert to uV/sqrt(Hz) 
	avg_beta = np.nanmean(beta_power)
	beta_per_trial = np.zeros(len(rtInd))

	# Compute power in band indicated by [f_low, f_high]
	bandf = np.floor(np.array([f_low, f_high])*512./1000) + 1		# find which frequency indices correspond to this band
	band_power = np.sqrt(np.sum(omniSpec[:, bandf[0]:bandf[1]],1))	# power is in lsb/sqrt(Hz)
	band_power = 0.0954*band_power 									# covert to uV/sqrt(Hz)
	avg_band_power = np.nanmean(band_power)
	band_power_per_trial = np.zeros(len(rtInd))

	# Find beta power computation that is closest to beginning of trial
	# Note: Hold is 400 - 600 ms in duration
	for i, trial_ind in enumerate(rtInd):
		power_index = np.argmin(np.abs(endFFT - rt_times[i]))
		if endFFT[power_index] > rt_times[i]:
			# if closest computation is after trial started, look one computation (256 ms) ahead
			beta_per_trial[i] = beta_power[power_index + 1]
			band_power_per_trial[i] = band_power[power_index + 1]
		else:
			# if closest computation is just before trial starts, look two computations (512 ms) ahead
			beta_per_trial[i] = beta_power[power_index + 2]
			band_power_per_trial[i] = band_power[power_index + 2]

	r, p = sp.stats.pearsonr(rt, beta_per_trial)
	print session[17:-4]
	print "Correlation (not normalized): %f (p = %f)" % (r, p)

	return rt, avg_rt, beta_per_trial, avg_beta, band_power_per_trial, avg_band_power

def AnalyzeOnlinePowerComputations_AcrossSessions(sessions, f_low, f_high):
	'''
	Aggragate analysis from AnalyzeOnlinePowerComputations across sessions.

	Input:
	- sessions: list of sessionX.mat files created by Andy
	'''
	rt_all = np.array([])
	norm_rt_all = np.array([])
	beta_per_trial_all = np.array([])
	norm_beta_per_trial_all = np.array([])
	band_power_per_trial_all = np.array([])
	norm_band_power_per_trial_all = np.array([])

	band_power_per_trial_all_good = np.array([])
	rt_all_good = np.array([])

	for session in sessions:
		rt, avg_rt, beta_per_trial, avg_beta, band_power_per_trial, avg_band_power = AnalyzeOnlinePowerComputations(session, f_low, f_high)
		rt_all = np.append(rt_all, rt)
		norm_rt_all = np.append(norm_rt_all, rt/avg_rt)
		beta_per_trial_all = np.append(beta_per_trial_all, beta_per_trial)
		band_power_per_trial_all = np.append(band_power_per_trial_all, band_power_per_trial)
		good_band_power = np.ravel(np.nonzero(np.less(band_power_per_trial,50))) 			# indices of non-spurious power computations (threshold of 50 is for high-gamma)
		band_power_per_trial_all_good = np.append(band_power_per_trial_all_good, band_power_per_trial[good_band_power])
		rt_all_good = np.append(rt_all_good, rt[good_band_power])
		# Powers normalized by within session average power in the designated band
		norm_beta_per_trial_all = np.append(norm_beta_per_trial_all, beta_per_trial/avg_beta)
		norm_band_power_per_trial_all = np.append(norm_band_power_per_trial_all, band_power_per_trial/avg_band_power)

	r, p = sp.stats.pearsonr(rt_all, beta_per_trial_all)
	r_norm, p_norm = sp.stats.pearsonr(rt_all, norm_beta_per_trial_all)
	r_band, p_band = sp.stats.pearsonr(rt_all, band_power_per_trial_all)
	r_norm_band, p_norm_band = sp.stats.pearsonr(rt_all, norm_band_power_per_trial_all)

	r_band_good, p_band_good = sp.stats.pearsonr(rt_all_good, band_power_per_trial_all_good)

	print "\n All Sessions Together:"
	print "Correlation - RT with Beta (not normalized): %f (p = %f)" % (r, p)
	print "Correlation - RT with Beta (normalized): %f (p = %f)" % (r_norm, p_norm)
	print "Correlation - RT with [%d,%d] (not normalized): %f (p = %f)" % (f_low,f_high, r_band, p_band)
	print "Correlation - RT with [%d,%d] (normalized): %f (p = %f)" % (f_low,f_high,r_norm_band, p_norm_band)
	print "\n Correlation - RT with [%d,%d] (not normalized, thresholded): %f (p = %f)" % (f_low,f_high, r_band_good, p_band_good)

	m, b = np.polyfit(rt_all, beta_per_trial_all,1)
	m_norm, b_norm = np.polyfit(rt_all, norm_beta_per_trial_all,1)
	m_band, b_band = np.polyfit(rt_all, band_power_per_trial_all,1)
	m_norm_band, b_norm_band = np.polyfit(rt_all, norm_band_power_per_trial_all,1)

	m_band_good, b_band_good = np.polyfit(rt_all_good, band_power_per_trial_all_good,1)

	plt.figure(0)
	plt.subplot(2,2,1)
	plt.plot(rt_all,beta_per_trial_all,'.')
	plt.plot(rt_all, m*rt_all + b, '-')
	plt.xlabel('Reaction Time (s)')
	plt.ylabel('Beta Power (uv/' + r'$\sqrt{Hz}$' + ')')
	plt.text(0.5, 110, r'$\rho$' + '= %.2f (p = %.2f)' % (r,p))

	plt.subplot(2,2,2)
	plt.plot(rt_all,norm_beta_per_trial_all,'.')
	plt.plot(rt_all, m_norm*rt_all + b_norm, '-')
	plt.xlabel('Reaction Time (s)')
	plt.ylabel('Normalized Beta Power')
	plt.text(0.5, 2, r'$\rho$' + '= %.2f (p = %.2f)' % (r_norm,p_norm))

	plt.subplot(2,2,3)
	plt.plot(rt_all,band_power_per_trial_all,'.')
	plt.plot(rt_all, m_band*rt_all + b_band, '-')
	plt.xlabel('Reaction Time (s)')
	plt.ylabel('Power (uv/' + r'$\sqrt{Hz}$' + '): [%d,%d] Hz' % (f_low,f_high))
	plt.text(0.5, 210, r'$\rho$' + '= %.2f (p = %.2f)' % (r_band,p_band))

	plt.subplot(2,2,4)
	plt.plot(rt_all,norm_band_power_per_trial_all,'.')
	plt.plot(rt_all, m_norm_band*rt_all + b_norm_band, '-')
	plt.xlabel('Reaction Time (s)')
	plt.ylabel('Normalized Band Power: [%d,%d] Hz' % (f_low,f_high))
	plt.text(0.5, 3.5, r'$\rho$' + '= %.2f (p = %.2f)' % (r_norm_band,p_norm_band))

	plt.figure(1)
	plt.plot(rt_all_good,band_power_per_trial_all_good,'.')
	plt.plot(rt_all_good, m_band_good*rt_all_good + b_band_good, '-')
	plt.xlabel('Reaction Time (s)')
	plt.ylabel('Power (uv/' + r'$\sqrt{Hz}$' + '): [%d,%d] Hz' % (f_low,f_high))
	plt.text(0.5, 17, r'$\rho$' + '= %.2f (p = %.2f)' % (r_band_good,p_band_good))
	plt.title('Thresholded Power Values ( < 50 uV /' + r'$\sqrt{Hz}$' + ')')

	plt.show()

	return

def AnalyzeOnlinePowerComputations_stimvsnostim(session, f_low, f_high):
	'''
	Method to extract online computed power recorded using OMNI and relate it to task variables. Outputs
	reaction time and beta power information, as well as power computed in a band specified by [f_low, f_high].
	Trials are divided into times when stim occurs during the hold and when it does not. Only considers successful
	trials.

	Input: 
	- session: .mat filename for .mat file generated by Andy's CompileSessionX code.
	- f_low: float representing the lower cutoff frequency of a window of interest (Hz)
	- f_high: float representing the higher cutoff frequency of a window of interest (Hz)
	'''
	
	# Load data and pull out information into arrays
	mat = sp.io.loadmat(session, squeeze_me = True)
	data = mat['session']
	behavior = np.ravel(data['behavior'])[0]			# behavior matrix, each column represents a trial, each row the timing of each event
	betaF = np.ravel(data['betaF'])[0] 					# FFT bins used for beta calculation
	ctrl = np.ravel(data['ctrl'])[0]					# control channel signal
	endFFT = np.ravel(data['endFFT'])[0]				# indices of the last sample of each computed FFT
	threshPower = np.ravel(data['threshPower'])[0]		# power threshold used for closed loop
	threshDeriv = np.ravel(data['threshDeriv'])[0]		# power derivative threshold for closed loop
	NFFT = np.ravel(data['NFFT'])[0]					# FFT size
	omniSpec = np.ravel(data['omniSpec'])[0]			# compiled online computed power spectrums (endFFT contains timing info)
	stim = np.ravel(data['stim'])[0]					# stim channel signal
	stimFlags = np.ravel(data['stimFlags'])[0]			# stim flags (0 = no stim, 1 = stim)
	stimEvents = np.ravel(data['stimEvents'])[0]		# indices of beginning and end of each stim pulse
	stimIdx = np.ravel(data['stimIdx'])[0]				# indices of all stim flags
	t = np.ravel(data['t'])[0]							# time vector
	trains = np.ravel(data['trains'])[0]				# indices of beginning and end of each stim pulse train
	trialSuccess = np.ravel(data['trialSuccess'])[0]	# indices of the successful trials
	centerHoldAll = np.ravel(data['centerHoldAll'])[0]  # center hold for ALL (successful and unsuccessful) trials
	rtInd = np.ravel(data['rtInd'])[0]					# indices of trial times (index into centerHoldAll) for reaction times
	rt = np.ravel(data['rt'])[0]						# reaction times

	stim_start = trains[:,0]
	stim_end = trains[0,1]

	# Find time samples corresponding to reaction time computations for successful trials
	rt_inds = np.array([ind for ind in range(rtInd[-1]+1) if (ind in rtInd)&(ind in trialSuccess)])  # trial indices of successful trials with computed RTs
	rt_all = np.zeros(len(centerHoldAll))
	rt_all[rtInd] = rt 																				# Array of RTs for all trials (successful and unsuccessful), with zeros where it was not computed
	rt_succ = rt_all[rt_inds]
	
	num_succ_trials = behavior.shape[1]
	begin_hold_times = np.array([behavior[0,ind] for ind in range(num_succ_trials) if trialSuccess[ind] in rt_inds])
	go_cue_times = np.array([behavior[2,ind] for ind in range(num_succ_trials) if trialSuccess[ind] in rt_inds])
	
	# Find indices of trials that stimulation occurred during the center hold
	ind_during_center_on = []
	ind_not_during_center_on = []

	for val in range(len(begin_hold_times)):
		# find closest stim to hold time
		stim_index = np.argmin(np.abs(stim_start - begin_hold_times[val]))
		# find time relative to hold time: positive is after, negative is before
		time_after_hold = stim_start[stim_index] - begin_hold_times[val]
		time_before_go_cue = stim_start[stim_index] - go_cue_times[val]
		is_during_center = (time_after_hold > -0.067)&(time_before_go_cue < 0)

		# build lists of indices for if stim occured when peripheral was shown or not
		if is_during_center:
			ind_during_center_on.append(val)
		else:
			ind_not_during_center_on.append(val)

	# Find average RT
	avg_rt = np.nanmean(rt_succ)

	# Compute beta power from spectral data saved
	beta_power = np.sqrt(np.sum(omniSpec[:, betaF[0]:betaF[1]],1))	# power is in lsb/sqrt(Hz)
	beta_power = 0.0954*beta_power 									# convert to uV/sqrt(Hz) 
	avg_beta = np.nanmean(beta_power)
	beta_per_trial = np.zeros(len(rt_inds))

	# Compute power in band indicated by [f_low, f_high]
	bandf = np.floor(np.array([f_low, f_high])*512./1000) + 1		# find which frequency indices correspond to this band
	band_power = np.sqrt(np.sum(omniSpec[:, bandf[0]:bandf[1]],1))	# power is in lsb/sqrt(Hz)
	band_power = 0.0954*band_power 									# covert to uV/sqrt(Hz)
	avg_band_power = np.nanmean(band_power)
	band_power_per_trial = np.zeros(len(rt_inds))

	# Find beta power computation that is closest to beginning of trial
	# Note: Hold is 400 - 600 ms in duration
	for i, trial_ind in enumerate(rt_inds):
		power_index = np.argmin(np.abs(endFFT - begin_hold_times[i]))
		if endFFT[power_index] > begin_hold_times[i]:
			# if closest computation is after trial started, look one computation (256 ms) ahead
			beta_per_trial[i] = beta_power[power_index + 1]
			band_power_per_trial[i] = band_power[power_index + 1]
		else:
			# if closest computation is just before trial starts, look two computations (512 ms) ahead
			beta_per_trial[i] = beta_power[power_index + 2]
			band_power_per_trial[i] = band_power[power_index + 2]

	# NEED TO SEPARATE TRIAL TYPES: STIM DURING CENTER HOLD VERSUS NOT
	beta_per_trial_stim_hold = beta_per_trial[ind_during_center_on]
	beta_per_trial_nostim_hold = beta_per_trial[ind_not_during_center_on]
	band_power_per_trial_stim_hold = band_power_per_trial[ind_during_center_on]
	band_power_per_trial_nostim_hold = band_power_per_trial[ind_not_during_center_on]
	rt_stim_hold = rt_succ[ind_during_center_on]
	rt_nostim_hold = rt_succ[ind_not_during_center_on]

	return rt_succ, beta_per_trial, avg_beta, band_power_per_trial, avg_band_power, beta_per_trial_stim_hold, beta_per_trial_nostim_hold, band_power_per_trial_stim_hold, band_power_per_trial_nostim_hold, rt_stim_hold, rt_nostim_hold

def AnalyzeOnlinePowerComputations_AcrossSessions_stimvsnostim(sessions, f_low, f_high):
	'''
	Aggragate analysis from AnalyzeOnlinePowerComputations_stimvsnostim across sessions.

	Input:
	- sessions: list of sessionX.mat files created by Andy
	'''
	rt_all = np.array([])
	beta_per_trial_all = np.array([])
	band_power_per_trial_all = np.array([])
	
	beta_per_trial_stim_hold_all = np.array([])
	beta_per_trial_nostim_hold_all = np.array([])
	band_power_per_trial_stim_hold_all = np.array([])
	band_power_per_trial_nostim_hold_all = np.array([])

	rt_stim_hold_all = np.array([])
	rt_nostim_hold_all = np.array([])

	for session in sessions:
		rt_succ, beta_per_trial, avg_beta, band_power_per_trial, avg_band_power, \
			beta_per_trial_stim_hold, beta_per_trial_nostim_hold, band_power_per_trial_stim_hold, \
			band_power_per_trial_nostim_hold, rt_stim_hold, rt_nostim_hold = AnalyzeOnlinePowerComputations_stimvsnostim(session, f_low, f_high)
		rt_all = np.append(rt_all, rt_succ)
		beta_per_trial_all = np.append(beta_per_trial_all, beta_per_trial)
		band_power_per_trial_all = np.append(band_power_per_trial_all, band_power_per_trial)
		beta_per_trial_stim_hold_all = np.append(beta_per_trial_stim_hold_all, beta_per_trial_stim_hold)
		beta_per_trial_nostim_hold_all = np.append(beta_per_trial_nostim_hold_all, beta_per_trial_nostim_hold)
		band_power_per_trial_stim_hold_all = np.append(band_power_per_trial_stim_hold_all, band_power_per_trial_stim_hold)
		band_power_per_trial_nostim_hold_all = np.append(band_power_per_trial_nostim_hold_all, band_power_per_trial_nostim_hold)
		rt_stim_hold_all = np.append(rt_stim_hold_all, rt_stim_hold)
		rt_nostim_hold_all = np.append(rt_nostim_hold_all, rt_nostim_hold)
		# Powers normalized by within session average power in the designated band
		#norm_beta_per_trial_all = np.append(norm_beta_per_trial_all, beta_per_trial/avg_beta)
		#norm_band_power_per_trial_all = np.append(norm_band_power_per_trial_all, band_power_per_trial/avg_band_power)

	# Find correlations
	r, p = sp.stats.pearsonr(rt_all, beta_per_trial_all)
	r_band, p_band = sp.stats.pearsonr(rt_all, band_power_per_trial_all)

	r_beta_stim, p_beta_stim = sp.stats.pearsonr(rt_stim_hold_all, beta_per_trial_stim_hold_all)
	r_band_stim, p_band_stim = sp.stats.pearsonr(rt_stim_hold_all, band_power_per_trial_stim_hold_all)

	r_beta_nostim, p_beta_nostim = sp.stats.pearsonr(rt_nostim_hold_all, beta_per_trial_nostim_hold_all)
	r_band_nostim, p_band_nostim = sp.stats.pearsonr(rt_nostim_hold_all, band_power_per_trial_nostim_hold_all)

	
	print "\n All Sessions Together:"
	print "Correlation - RT with Beta (not normalized): %f (p = %f)" % (r, p)
	print "Correlation - RT with [%d,%d] (not normalized): %f (p = %f)" % (f_low,f_high, r_band, p_band)
	print "Correlation - RT with Beta (stim during Center Hold): %f (p = %f)" % (r_beta_stim, p_beta_stim)
	print "Correlation - RT with [%d,%d] (stim during Center Hold): %f (p = %f)" % (f_low,f_high, r_band_stim, p_band_stim)
	print "Correlation - RT with Beta (no stim during Center Hold): %f (p = %f)" % (r_beta_nostim, p_beta_nostim)
	print "Correlation - RT with [%d,%d] (no stim during Center Hold): %f (p = %f)" % (f_low,f_high, r_band_nostim, p_band_nostim)

	# Do linear regression for all trials
	x = beta_per_trial_all
	x.reshape(len(x),1)
	print x.shape
	#x = np.transpose(x)
	#x = np.hstack((x, np.ones([len(beta_per_trial_all),1]))) 	# use this in place of add_constant which doesn't work when constant Q values are used
	x = sm.add_constant(x, prepend=False)
	y = rt_all

	print "\n Regression for RT"
	model_glm = sm.OLS(y,x)
	fit_glm = model_glm.fit()
	beta_reg = fit_glm.params[0]
	const_reg = fit_glm.params[1]
	y_fit = beta_reg*beta_per_trial_all + const_reg
	y_err = np.nanmean((y_fit - y)**2)
	print fit_glm.summary()

	# Find linear fits for plotting
	m, b = np.polyfit(rt_all, beta_per_trial_all,1)
	m_band, b_band = np.polyfit(rt_all, band_power_per_trial_all,1)
	m_beta_stim, b_beta_stim = np.polyfit(rt_stim_hold_all, beta_per_trial_stim_hold_all,1)
	m_band_stim, b_band_stim = np.polyfit(rt_stim_hold_all, band_power_per_trial_stim_hold_all,1)
	m_beta_nostim, b_beta_nostim = np.polyfit(rt_nostim_hold_all, beta_per_trial_nostim_hold_all,1)
	m_band_nostim, b_band_nostim = np.polyfit(rt_nostim_hold_all, band_power_per_trial_nostim_hold_all,1)
	

	plt.figure(0)
	plt.subplot(2,3,1)
	plt.plot(rt_all,beta_per_trial_all,'.')
	plt.plot(rt_all, m*rt_all + b, '-')
	plt.xlabel('Reaction Time (s)')
	plt.ylabel('Beta Power (uv/' + r'$\sqrt{Hz}$' + ')')
	plt.title('All Trials')
	plt.text(0.6, 70, r'$\rho$' + ' = %.3f (p = %.3f)' % (r,p))

	plt.subplot(2,3,4)
	plt.plot(rt_all,band_power_per_trial_all,'.')
	plt.plot(rt_all, m_band*rt_all + b_band, '-')
	plt.xlabel('Reaction Time (s)')
	plt.ylabel('Power (uv/' + r'$\sqrt{Hz}$' + '): [%d,%d] Hz' % (f_low,f_high))
	plt.title('All Trials')
	plt.text(0.6, 16, r'$\rho$' + ' = %.3f (p = %.3f)' % (r_band,p_band))

	plt.subplot(2,3,2)
	plt.plot(rt_stim_hold_all,beta_per_trial_stim_hold_all,'.')
	plt.plot(rt_stim_hold_all, m_beta_stim*rt_stim_hold_all + b_beta_stim, '-')
	plt.xlabel('Reaction Time (s)')
	plt.ylabel('Beta Power (uv/' + r'$\sqrt{Hz}$' + ')')
	plt.title('Stim during Hold')
	plt.text(0.45, 70, r'$\rho$' + ' = %.3f (p = %.3f)' % (r_beta_stim,p_beta_stim))

	plt.subplot(2,3,5)
	plt.plot(rt_stim_hold_all,band_power_per_trial_stim_hold_all,'.')
	plt.plot(rt_stim_hold_all, m_band_stim*rt_stim_hold_all + b_band_stim, '-')
	plt.xlabel('Reaction Time (s)')
	plt.ylabel('Power (uv/' + r'$\sqrt{Hz}$' + '): [%d,%d] Hz' % (f_low,f_high))
	plt.title('Stim during Hold')
	plt.text(0.45, 14, r'$\rho$' + ' = %.3f (p = %.3f)' % (r_band_stim,p_band_stim))

	plt.subplot(2,3,3)
	plt.plot(rt_nostim_hold_all,beta_per_trial_nostim_hold_all,'.')
	plt.plot(rt_nostim_hold_all, m_beta_nostim*rt_nostim_hold_all + b_beta_nostim, '-')
	plt.xlabel('Reaction Time (s)')
	plt.ylabel('Beta Power (uv/' + r'$\sqrt{Hz}$' + ')')
	plt.title('No Stim during Hold')
	plt.text(0.6, 70, r'$\rho$' + ' = %.3f (p = %.3f)' % (r_beta_nostim,p_beta_nostim))

	plt.subplot(2,3,6)
	plt.plot(rt_nostim_hold_all,band_power_per_trial_nostim_hold_all,'.')
	plt.plot(rt_nostim_hold_all, m_band_nostim*rt_nostim_hold_all + b_band_nostim, '-')
	plt.xlabel('Reaction Time (s)')
	plt.ylabel('Power (uv/' + r'$\sqrt{Hz}$' + '): [%d,%d] Hz' % (f_low,f_high))
	plt.title('No Stim during Hold')
	plt.text(0.6, 16, r'$\rho$' + ' = %.3f (p = %.3f)' % (r_band_nostim,p_band_nostim))

	plt.show()

	return beta_reg, const_reg, y, y_fit, y_err